{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgQD_db1E37Y",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Robustness metric and classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "NznfsP5yE37Z",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "FI = feature importance\n",
    "FS = Feature selection\n",
    "FG = faking good\n",
    "FB = faking bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "TQl7xrsgE37a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prof questions and thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "KhvbA1VzE37a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1) As regards to the importance of the features (items of the tests). After feature selection, Do diverse ML models with equal accuracy index different items as more important? (in discriminating Honest vs dishonest responses)\n",
    "\n",
    "\tATTENTION: take note that in some datasets the Dishonest is fake-good ad in others i fake-bad.\n",
    "Is there a difference in response to question n. 1 in relation to fake-good/Fake bad?\n",
    "\n",
    "\n",
    "2) Does the response to question 1  change according to the dataset?\n",
    "\n",
    "\n",
    "3) Which is the accuracy  in classification that is achieved using only the best 20% of the items?\n",
    "Which is the amount of shrinkage with respect to the original test?  (e.g. with the full test of 100 Random forest lands an accuracy of 92% with the best 20 items the accuracy drops to 88%)\n",
    " If you take the best 20% of the items of a given test,  which is the concordance between classifiers in spotting the 20% best items? (e.g. in a test of 100 items only 10 among the best 20 are the same for different classifiers ; SVM Random Forest, knn etc.). You may use the rank order correlation\n",
    "\n",
    "\n",
    "4) Are model agnostic feature selection (e.g. permutation importance or PCA) procedures better than model dependent\n",
    "\n",
    "\n",
    "5) Are psychometric inspired items selection better than model based item selection?\n",
    "Psychometric inspired item selection are usually based on PCA or factor analysis. With the best items selected on the basis of factor loadings.\n",
    "\n",
    "\n",
    "6) Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "qyptzp4BxMor",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset\n",
    "- Short DT: 2 dataset faking good (27 features x ~400 osservazioni)\n",
    "- PRMQ: 1 dataset faking bad (16 feature x 1400 osservazioni)\n",
    "- PCL: 1 dataset faking bad (20 feature x 400 osservazioni)\n",
    "- NAQ: 1 dataset faking bad (21 feature x 800 osservazioni)\n",
    "- GAD: 1 dataset faking bad (16 feature x 1100 osservazioni)\n",
    "- PID: 1 dataset faking bad (220 feature x 400 osservazioni)\n",
    "- Short PID: 1 dataset faking bad (25 feature x 1100 osservazioni)\n",
    "- PRFQ: 1 dataset faking good (18 feature x 678 osservazioni)\n",
    "- IESR: 1 dataset faking bad (22 feature x 378 osservazioni)\n",
    "- NEO PI: 1 dataset faking good (30 feature x 80000 osservazioni)\n",
    "- DDDT: 1 dataset faking good (15 feature x 1000 osservazioni)\n",
    "- IADQ: 1 dataset faking bad (9 feature x 450 osservazioni)\n",
    "- BF: 3 dataset faking good (10 feature x ~230 osservazioni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "V5Rv8wp3E37b",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFuh2KUeE37b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jCooW0QJE37c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "N_CV = 5\n",
    "REMOVE_COL = True\n",
    "PERCENTAGE_FEATURE_TO_REMOVE = .8\n",
    "COV_THRESHOLD = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VuBkNv2fE37c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"1shortDT//DT_df_CC.csv\")\n",
    "#dataset = pd.read_csv(\"1shortDT//DT_df_JI.csv\")\n",
    "#dataset = pd.read_csv(\"2PRMQ//PRMQ_df.csv\") # senza correlazioni toglie tutte le domande meno che due...\n",
    "#dataset = pd.read_csv(\"3PCL//PCL5_df.csv\") # senza correlazioni toglie tutte le domande meno che due...\n",
    "#dataset = pd.read_csv(\"6PID5//PID5_df.csv\") # tante feature, si spacca parecchia roba\n",
    "#dataset = pd.read_csv(\"7shortPID5//sPID-5_df.csv\") # dimostra che KPCA honest only non è una buona scelta\n",
    "#dataset = pd.read_csv(\"8PRFQ//PRFQ_df.csv\")\n",
    "#dataset = pd.read_csv(\"9IESR//IESR_df.csv\") # ha molta covarianza, probabilmente la più alta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZtlyQxpE37c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = np.array(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "A7u1jXhEE37c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Assumption: the variables are monotonic wrt the response, so no \"to categorical\" is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7acPaZ4hE37c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = data[:,:-1].astype(float) # StandardScaler().fit_transform(data[:,:-1].astype(float))\n",
    "y = np.zeros(data.shape[0])\n",
    "y[data[:,-1] == \"H\"] = 1\n",
    "original_dataset_size = X.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "hhrmUz5dE37c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "h7oWc5raE37c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The more separated they are, the simpler is the task, as for example a Naybe bayesian yould capture the difference easily since assumes independence in the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "aoXyLHEKE37d",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Features marginal distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1093
    },
    "id": "Z-eWciriE37d",
    "outputId": "fe756175-69e2-475b-b71e-3a7959038312",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axs = plt.subplots(9,3, figsize=(15,15))\n",
    "fig.tight_layout(pad=1.5)\n",
    "for i, (ax, label_name) in enumerate(zip(axs.flatten(), dataset.columns[:-1])):\n",
    "\tsns.kdeplot(X[np.where(y == 1)][:,i], ax=ax)\n",
    "\tsns.kdeplot(X[np.where(y == 0)][:,i], ax=ax)\n",
    "\tax.set_xlim(1,5)\n",
    "\tax.set_xticks(range(1,6))\n",
    "\tax.set_title(label_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ks3bGE6ME37d",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Features mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "7HwB0EvDE37d",
    "outputId": "45cc8c2c-ab35-401a-f01d-aa823469e9e5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "m1 = X[y==1].mean(axis=0)\n",
    "m0 = X[y==0].mean(axis=0)\n",
    "e1 = X[y==1].std(axis=0)\n",
    "e0 = X[y==0].std(axis=0)\n",
    "plt.fill_between(range(1, X.shape[-1]+1), m1-e1, m1+e1, alpha=0.5, label=\"_nolegend_\")\n",
    "plt.plot(range(1, X.shape[-1]+1), m1)\n",
    "\n",
    "plt.fill_between(range(1, X.shape[-1]+1), m0-e0, m0+e0, alpha=0.5, label=\"_nolegend_\")\n",
    "plt.plot(range(1, X.shape[-1]+1), m0)\n",
    "\n",
    "plt.ylabel(\"Mean Likert value\")\n",
    "plt.xlabel(\"Feature (Question number)\")\n",
    "plt.legend([\"Honest\", \"Dishonest\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Uncorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if REMOVE_COL:\n",
    "\twhile X.shape[-1] > 1:\n",
    "\t\tcorr = stats.spearmanr(X).correlation\n",
    "\t\tif isinstance(corr, np.float64):\n",
    "\t\t\tcorr = np.array([corr])\n",
    "\t\tcorrelations = np.where(np.sum(\n",
    "\t\t\tnp.abs(corr*(1-np.identity(len(corr)))) > COV_THRESHOLD\n",
    "\t\t\t, axis=1) > 0)[0]\n",
    "\t\tif len(correlations) > 0:\n",
    "\t\t\tX = X[:,np.where(np.arange(0, X.shape[-1]) != correlations[0])[0]]\n",
    "\t\t\tprint(f\"deleting {correlations[0]}\")\n",
    "\t\telse:\n",
    "\t\t\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X.shape[-1],int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if X.shape[-1] < int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE)):\n",
    "\traise Exception(\"not enough feature, increase covariance threshold or decrease % features to remove\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "k3LP8PTlE37e",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## FS with model dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIA8CjboE37e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# with model-dependent feature importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# ensemble\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKXYWErTE37e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# take feature importance and transform them to distribution (positive and sum to 1)\n",
    "def to_distribution(values):\n",
    "\tvalues = np.abs(values)\n",
    "\treturn values / np.sum(values) if np.sum(values) > 0 else np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTI-MpK6E37e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(random_state=0).fit(X,y)\n",
    "logistic_regression = LogisticRegression(random_state=0).fit(X,y)\n",
    "gradient_boosting = GradientBoostingClassifier(random_state=0).fit(X,y)\n",
    "svm = SVC(kernel = \"linear\",random_state=0).fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 859
    },
    "id": "PdyUEqi4E37e",
    "outputId": "b32126e4-ea74-4ad6-eaf1-8cb2d69f0fa7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_bar = np.arange(1, X.shape[-1]+1)\n",
    "width = .2\n",
    "plt.figure(dpi=200, figsize=(20,5))\n",
    "plt.bar(x_bar, to_distribution(random_forest.feature_importances_), width=width, color='yellow')\n",
    "plt.bar(x_bar+width, to_distribution(logistic_regression.coef_.squeeze()), width=width, color='greenyellow')\n",
    "plt.bar(x_bar+width*2, to_distribution(gradient_boosting.feature_importances_), width=width, color='yellowgreen')\n",
    "plt.bar(x_bar+width*3, to_distribution(svm.coef_.squeeze()), width=width, color='olive')\n",
    "plt.ylabel(\"Feature importance (%)\")\n",
    "plt.xlabel(\"Question number\")\n",
    "plt.legend([\"Random Forest\", \"Logistic Regression\", \"Gradient Boosting\", \"SVM\"])\n",
    "_ = plt.xticks(x_bar + width + width / 2, x_bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "DuHqz4MYE37e",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Accuracy using all the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "WGsJsGigE37e",
    "outputId": "ce5f1aaa-add8-4aa7-c24f-edb05bcd614e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "Accuracies:\n",
    "Random Forest: {(cross_val_score(random_forest, X, y, cv=N_CV).mean()*100).round(2)}%\n",
    "Logistic Regression: {(cross_val_score(logistic_regression, X, y, cv=N_CV).mean()*100).round(2)}%\n",
    "Gradient Boosting: {(cross_val_score(gradient_boosting, X, y, cv=N_CV).mean()*100).round(2)}%\n",
    "SVC: {(cross_val_score(svm, X, y, cv=N_CV).mean()*100).round(2)}%\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "uqcyNm7hE37f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Accuracy using only top 20% specific for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "zniSqmRvE37f",
    "outputId": "44f4dba6-60cb-4575-e79f-bb8abae645d2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_forest_top_20_features = np.argsort(to_distribution(random_forest.feature_importances_))[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "logistic_regression_top_20_features = np.argsort(to_distribution(logistic_regression.coef_.squeeze()))[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "gradient_boosting_top_20_features = np.argsort(to_distribution(gradient_boosting.feature_importances_))[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "svm_top_20_features = np.argsort(to_distribution(svm.coef_.squeeze()))[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "\n",
    "print(f\"\"\"\n",
    "Accuracies:\n",
    "Random Forest: {(cross_val_score(random_forest, X[:,random_forest_top_20_features], y, cv=N_CV).mean()*100).round(2)}%\n",
    "Logistic Regression: {(cross_val_score(logistic_regression, X[:,logistic_regression_top_20_features], y, cv=N_CV).mean()*100).round(2)}%\n",
    "Gradient Boosting: {(cross_val_score(gradient_boosting, X[:,gradient_boosting_top_20_features], y, cv=N_CV).mean()*100).round(2)}%\n",
    "SVC: {(cross_val_score(svm, X[:,svm_top_20_features], y, cv=N_CV).mean()*100).round(2)}%\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "r6xm9adSE37f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "taking top 20%, which is the amount of concordance between them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "RraE-4vjE37f",
    "outputId": "cbde3e5a-e6d5-4087-caf4-c66a2b97e7b4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.array((random_forest_top_20_features, logistic_regression_top_20_features, gradient_boosting_top_20_features, svm_top_20_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "rfXEkeJtE37f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Change in accuracy considering only top N features ordered by importance wrt each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "egkDyFcOE37f",
    "outputId": "49091789-9160-4d40-b9a1-b48d6700dc7c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_forest_sorted_features = np.argsort(to_distribution(random_forest.feature_importances_)).squeeze()\n",
    "logistic_regression_sorted_features = np.argsort(to_distribution(logistic_regression.coef_.squeeze())).squeeze()\n",
    "gradient_boosting_sorted_features = np.argsort(to_distribution(gradient_boosting.feature_importances_)).squeeze()\n",
    "svm_sorted_features = np.argsort(to_distribution(svm.coef_.squeeze())).squeeze()\n",
    "accuracies_filtered_singularly = []\n",
    "X_filtered_rf = np.copy(X)\n",
    "X_filtered_lg = np.copy(X)\n",
    "X_filtered_svm = np.copy(X)\n",
    "X_filtered_gb = np.copy(X)\n",
    "for i in trange(0, X.shape[-1]):\n",
    "\n",
    "\t# find accuracy\n",
    "\taccuracies_filtered_singularly.append([\n",
    "\t\t(cross_val_score(random_forest, X_filtered_rf, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(logistic_regression, X_filtered_lg, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(gradient_boosting, X_filtered_gb, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(svm, X_filtered_svm, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t])\n",
    "\n",
    "\t# train to get feature importance\n",
    "\trandom_forest.fit(X_filtered_rf, y)\n",
    "\tlogistic_regression.fit(X_filtered_lg, y)\n",
    "\tgradient_boosting.fit(X_filtered_gb, y)\n",
    "\tsvm.fit(X_filtered_svm, y)\n",
    "\n",
    "\t# from feature importance to distribution\n",
    "\trandom_forest_feature_importance = to_distribution(random_forest.feature_importances_)\n",
    "\tlogistic_regression_feature_importance = to_distribution(logistic_regression.coef_.squeeze())\n",
    "\tgradient_boosting_feature_importance = to_distribution(gradient_boosting.feature_importances_)\n",
    "\tsvm_feature_importance = to_distribution(svm.coef_.squeeze())\n",
    "\n",
    " \t# remove the least important feature for each model\n",
    "\tX_filtered_rf = X_filtered_rf[:, np.argsort(random_forest_feature_importance)[::-1][:-1]]\n",
    "\tX_filtered_lg = X_filtered_lg[:, np.argsort(logistic_regression_feature_importance)[::-1][:-1]]\n",
    "\tX_filtered_gb = X_filtered_gb[:, np.argsort(gradient_boosting_feature_importance)[::-1][:-1]]\n",
    "\tX_filtered_svm = X_filtered_svm[:, np.argsort(svm_feature_importance)[::-1][:-1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "OF_NxlvJE37f",
    "outputId": "197926e8-0726-452e-c4aa-c36c2ef39396",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_singularly[::-1])\n",
    "plt.xticks(np.arange(1, X.shape[-1]+1, 3))\n",
    "plt.legend([\"Random forest\", \"Logistic regression\", \"Gradient boosting\", \"SVM\"])\n",
    "plt.xlabel(\"Number of top feature considered in the training\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "nRBD5XgtE37f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Feature elimination based on aggregate importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "U2S9vW30E37g",
    "outputId": "5c75a080-6672-4adc-d953-544376384c38",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_filtered_together = []\n",
    "X_filtered = np.copy(X)\n",
    "feature_importance = []\n",
    "features_selected_at_each_step = [np.arange(0, X.shape[-1]).tolist()]\n",
    "features_importance_at_each_step = []\n",
    "for i in trange(X.shape[-1], 0, -1):\n",
    "\taccuracies_filtered_together.append([\n",
    "\t\t(cross_val_score(random_forest, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(logistic_regression, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(gradient_boosting, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(svm, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t])\n",
    "\tif i > 1 :\n",
    "\t\trandom_forest.fit(X_filtered, y)\n",
    "\t\tlogistic_regression.fit(X_filtered, y)\n",
    "\t\tgradient_boosting.fit(X_filtered, y)\n",
    "\t\tsvm.fit(X_filtered, y)\n",
    "\n",
    "\t\trandom_forest_feature_importance = to_distribution(random_forest.feature_importances_)\n",
    "\t\tlogistic_regression_feature_importance = to_distribution(logistic_regression.coef_.squeeze())\n",
    "\t\tgradient_boosting_feature_importance = to_distribution(gradient_boosting.feature_importances_)\n",
    "\t\tsvm_feature_importance = to_distribution(svm.coef_.squeeze())\n",
    "\n",
    "\t\tsummed_importance = np.sum([\n",
    "\t\t\trandom_forest_feature_importance, logistic_regression_feature_importance,\n",
    "\t\t\tgradient_boosting_feature_importance, svm_feature_importance\n",
    "\t\t], axis=0)\n",
    "\n",
    "\t\tfeature_importance.append(summed_importance)\n",
    "\t\tselected_features = np.argsort(summed_importance)[::-1][:-1]\n",
    "\t\tfeatures_selected_at_each_step.append(selected_features)\n",
    "\t\tfeatures_importance_at_each_step.append([\n",
    "\t\t\trandom_forest_feature_importance, logistic_regression_feature_importance,\n",
    "\t\t\tgradient_boosting_feature_importance, svm_feature_importance\n",
    "\t\t])\n",
    "\n",
    "\t\tX_filtered = X_filtered[:,selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668
    },
    "id": "g9ahScfyE37h",
    "outputId": "d657d63c-29de-499e-9966-543f4bfd3384",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 2, figsize=(10,5))\n",
    "axs[0].set_title(\"Common feature selection\")\n",
    "axs[0].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_together[::-1])\n",
    "axs[0].set_xticks(np.arange(1, X.shape[-1]+1, 3))\n",
    "axs[0].legend([\"Random forest\", \"Logistic regression\", \"Gradient boosting\", \"SVM\"])\n",
    "axs[0].set_xlabel(\"Number of top feature considered in the training\")\n",
    "axs[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "axs[1].set_title(\"Individual feature selection\")\n",
    "axs[1].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_singularly[::-1])\n",
    "axs[1].set_xticks(np.arange(1, X.shape[-1]+1, 3))\n",
    "axs[1].legend([\"Random forest\", \"Logistic regression\", \"Gradient boosting\", \"SVM\"])\n",
    "axs[1].set_xlabel(\"Number of top feature considered in the training\")\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "6Ze3Vr8kxMox",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$$\n",
    "Index = \\frac{1}{n \\choose 2} \\sum_{(r_0, r_1) \\subset R} Index(r_0, r_1)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "nyJYxIuzE37h",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Using an Voting ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Ob7WXSDE37h",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "voting_ensemble = VotingClassifier([\n",
    "\t('rf', random_forest),\n",
    "\t('lr', logistic_regression),\n",
    "\t('gb', gradient_boosting)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMDoFiFME37h",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_voting_ensemble = []\n",
    "for features in features_selected_at_each_step:\n",
    "\taccuracies_voting_ensemble.append((cross_val_score(voting_ensemble, X[:,features], y, cv=N_CV, n_jobs=-1).mean()*100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePqboq0mE37h",
    "outputId": "e51c3d3c-069c-4d61-8572-3b94356dc8d9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 3, figsize=(30,5))\n",
    "axs[0].set_title(\"Common feature selection\")\n",
    "axs[0].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_together[::-1])\n",
    "axs[0].set_xticks(np.arange(1, X.shape[-1]+1, 3))\n",
    "axs[0].legend([\"Random forest\", \"Logistic regression\", \"Gradient boosting\", \"SVM\"])\n",
    "axs[0].set_xlabel(\"Number of top feature considered in the training\")\n",
    "axs[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "axs[1].set_title(\"Individual feature selection\")\n",
    "axs[1].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_singularly[::-1])\n",
    "axs[1].set_xticks(np.arange(1, X.shape[-1]+1, 3))\n",
    "axs[1].legend([\"Random forest\", \"Logistic regression\", \"Gradient boosting\", \"SVM\"])\n",
    "axs[1].set_xlabel(\"Number of top feature considered in the training\")\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "\n",
    "axs[2].set_title(\"Voting Ensemble Accuracy\")\n",
    "axs[2].plot(np.arange(1, X.shape[-1]+1), accuracies_voting_ensemble[::-1])\n",
    "axs[2].plot(np.arange(1, X.shape[-1]+1), np.mean(accuracies_filtered_together[::-1], axis=-1))\n",
    "axs[2].legend([\"Voting ensemble accuracy\",\"Average single model accuracy\"])\n",
    "axs[2].set_xlabel(\"Number of top feature considered in the training\")\n",
    "axs[2].set_ylabel(\"Accuracy\")\n",
    "_ = axs[2].set_xticks(np.arange(1, X.shape[-1]+1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ecdjL1e9E37i",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Using Stacking Ensemble (meta-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jRvg0uljE37i",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stacking_ensemble = StackingClassifier(estimators=[\n",
    "\t('rf', RandomForestClassifier(n_estimators=100)),\n",
    "\t('gb', gradient_boosting),\n",
    "], final_estimator=logistic_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mELJx2bE37i",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_stacking_ensemble = []\n",
    "for features in features_selected_at_each_step:\n",
    "\taccuracies_stacking_ensemble.append((cross_val_score(stacking_ensemble, X[:,features], y, cv=N_CV, n_jobs=-1).mean()*100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8aPr_JAeE37i",
    "outputId": "e36a4767-9545-46bd-ba05-3ebb7bb5bb7a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 3, figsize=(30,5))\n",
    "axs[0].set_title(\"Common feature selection\")\n",
    "axs[0].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_together[::-1])\n",
    "axs[0].set_xticks(np.arange(1, X.shape[-1]+1, 3))\n",
    "axs[0].legend([\"Random forest\", \"Logistic regression\", \"Gradient boosting\", \"SVM\"])\n",
    "axs[0].set_xlabel(\"Number of top feature considered in the training\")\n",
    "axs[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "axs[1].set_title(\"Individual feature selection\")\n",
    "axs[1].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_singularly[::-1])\n",
    "axs[1].set_xticks(np.arange(1, X.shape[-1]+1, 3))\n",
    "axs[1].legend([\"Random forest\", \"Logistic regression\", \"Gradient boosting\", \"SVM\"])\n",
    "axs[1].set_xlabel(\"Number of top feature considered in the training\")\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "\n",
    "axs[2].set_title(\"Ensemble Accuracy\")\n",
    "axs[2].plot(np.arange(1, X.shape[-1]+1), np.array(accuracies_filtered_together[::-1])[:,0])\n",
    "axs[2].plot(np.arange(1, X.shape[-1]+1), accuracies_voting_ensemble[::-1])\n",
    "axs[2].plot(np.arange(1, X.shape[-1]+1), accuracies_stacking_ensemble[::-1])\n",
    "axs[2].plot(np.arange(1, X.shape[-1]+1), np.mean(accuracies_filtered_together[::-1], axis=-1))\n",
    "axs[2].legend([\"Random Forest\", \"Voting ensemble accuracy\",\"Stacking ensemble accuracy\",\"Average single model accuracy\"])\n",
    "axs[2].set_xlabel(\"Number of top feature considered in the training\")\n",
    "axs[2].set_ylabel(\"Accuracy\")\n",
    "_ = axs[2].set_xticks(np.arange(1, X.shape[-1]+1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "vBTCFKu8E37i",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## FS with model independent with classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "hBv3fYHnE37i",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "here:\n",
    "  \t- permutation importance\n",
    "  \t- partial dependence\n",
    "  \t- leave one covariate out\n",
    "  \t- Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "YL-NyIaSxMoy",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "here and in the unsupervised section we can use more models as the techniques used to estimate the importance are model agnostic, so we don't need a clear estimator inside the model (reason why I just used those 4 models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8LMIUlDxMoy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Extra, per iniziare confronteremo solo quelli con la baseline di sklearn (quelli fatti nella sezione prima)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier # categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.inspection import partial_dependence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "dS7uyXEJxMoy",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Permutation importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### PI with singular classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9THjd1LxMoy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Features importances plot for each classifier\n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=0).fit(X,y)\n",
    "logistic_regression = LogisticRegression(random_state=0).fit(X,y)\n",
    "gradient_boosting = GradientBoostingClassifier(random_state=0).fit(X,y)\n",
    "svm = SVC(kernel = \"linear\",random_state=0).fit(X,y)\n",
    "\n",
    "PI_random_forest = permutation_importance(random_forest, X, y, n_repeats=10, random_state=0)\n",
    "PI_logistic_regression = permutation_importance(logistic_regression, X, y, n_repeats=10, random_state=0)\n",
    "PI_gradient_boosting = permutation_importance(gradient_boosting, X, y, n_repeats=10, random_state=0)\n",
    "PI_svm = permutation_importance(svm, X, y, n_repeats=10, random_state=0)\n",
    "\n",
    "PI_random_forest_importance = to_distribution(PI_random_forest.importances_mean)\n",
    "PI_logistic_regression_importance = to_distribution(PI_logistic_regression.importances_mean)\n",
    "PI_gradient_boosting_importance = to_distribution(PI_gradient_boosting.importances_mean)\n",
    "PI_svm_importance = to_distribution(PI_svm.importances_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "cLz6pM6KxMoy",
    "outputId": "716f320e-7ad0-456d-d68c-cbb1f4b253f4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_bar = np.arange(1, X.shape[-1]+1)\n",
    "width = .2\n",
    "_, axs = plt.subplots(2, 2, figsize=(20,10))\n",
    "axs[0][0].set_title(\"Random Forest feature selection\")\n",
    "axs[0][0].bar(x_bar, PI_random_forest_importance, width=width)\n",
    "axs[0][0].bar(x_bar+width, to_distribution(random_forest.feature_importances_), width=width)\n",
    "axs[0][0].legend([\"PI importances\", \"Model importances\"])\n",
    "axs[0][1].set_title(\"Logistic regression feature selection\")\n",
    "axs[0][1].bar(x_bar, PI_logistic_regression_importance, width=width)\n",
    "axs[0][1].bar(x_bar+width, to_distribution(logistic_regression.coef_.squeeze()), width=width)\n",
    "axs[0][1].legend([\"PI importances\", \"Model importances\"])\n",
    "axs[1][0].set_title(\"Gradient Boosting feature selection\")\n",
    "axs[1][0].bar(x_bar, PI_gradient_boosting_importance, width=width)\n",
    "axs[1][0].bar(x_bar+width, to_distribution(gradient_boosting.feature_importances_), width=width)\n",
    "axs[1][0].legend([\"PI importances\", \"Model importances\"])\n",
    "axs[1][1].set_title(\"SVM feature selection\")\n",
    "axs[1][1].bar(x_bar, PI_svm_importance, width=width)\n",
    "axs[1][1].bar(x_bar+width, to_distribution(svm.coef_.squeeze()), width=width)\n",
    "axs[1][1].legend([\"PI importances\", \"Model importances\"])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "KLMTOk-KxMoz",
    "outputId": "9568caad-bc48-448a-f750-224ebfa7022f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Accuracies for each classifier\n",
    "\n",
    "accuracies_filtered_PI = []\n",
    "X_filtered_rf = np.copy(X)\n",
    "X_filtered_lr = np.copy(X)\n",
    "X_filtered_gb = np.copy(X)\n",
    "X_filtered_svm = np.copy(X)\n",
    "\n",
    "for i in trange(0, X.shape[-1]):\n",
    "\n",
    "  accuracies_filtered_PI.append([\n",
    "\t\t(cross_val_score(random_forest, X_filtered_rf, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(logistic_regression, X_filtered_lr, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(gradient_boosting, X_filtered_gb, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(svm, X_filtered_svm, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t])\n",
    "\n",
    "  random_forest_filtered = RandomForestClassifier(random_state=0).fit(X_filtered_rf,y)\n",
    "  logistic_regression_filtered = LogisticRegression(random_state=0).fit(X_filtered_lr,y)\n",
    "  gradient_boosting_filtered = GradientBoostingClassifier(random_state=0).fit(X_filtered_gb,y)\n",
    "  svm_filtered = SVC(kernel = \"linear\", random_state=0).fit(X_filtered_svm,y)\n",
    "\n",
    "  PI_random_forest_filtered = permutation_importance(random_forest_filtered, X_filtered_rf, y, n_repeats=10, random_state=0)\n",
    "  PI_logistic_regression_filtered = permutation_importance(logistic_regression_filtered, X_filtered_lr, y, n_repeats=10, random_state=0)\n",
    "  PI_gradient_boosting_filtered = permutation_importance(gradient_boosting_filtered, X_filtered_gb, y, n_repeats=10, random_state=0)\n",
    "  PI_svm_filtered = permutation_importance(svm_filtered, X_filtered_svm, y, n_repeats=10, random_state=0)\n",
    "\n",
    "  PI_random_forest_filtered_feature_importance = to_distribution(PI_random_forest_filtered.importances_mean)\n",
    "  PI_logistic_regression_filtered_feature_importance = to_distribution(PI_logistic_regression_filtered.importances_mean)\n",
    "  PI_gradient_boosting_filtered_feature_importance = to_distribution(PI_gradient_boosting_filtered.importances_mean)\n",
    "  PI_svm_filtered_feature_importance = to_distribution(PI_svm_filtered.importances_mean)\n",
    "\n",
    "  X_filtered_rf = X_filtered_rf[:, np.argsort(PI_random_forest_filtered_feature_importance)[::-1][:-1]]\n",
    "  X_filtered_lr = X_filtered_lr[:, np.argsort(PI_logistic_regression_filtered_feature_importance)[::-1][:-1]]\n",
    "  X_filtered_gb = X_filtered_gb[:, np.argsort(PI_gradient_boosting_filtered_feature_importance)[::-1][:-1]]\n",
    "  X_filtered_svm = X_filtered_svm[:, np.argsort(PI_svm_filtered_feature_importance)[::-1][:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-sgkBh6xMoz",
    "outputId": "be3f6572-ab74-4c63-f25f-6643e7e0d585",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_filtered_singularly = np.array(accuracies_filtered_singularly)\n",
    "accuracies_filtered_PI = np.array(accuracies_filtered_PI)\n",
    "\n",
    "_, axs = plt.subplots(1, 4, figsize=(15,5))\n",
    "for i in range(4):\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_singularly[:,i][::-1])\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_PI[:,i][::-1])\n",
    "\taxs[i].set_xticks(np.arange(1, X.shape[-1]+1, 3))\n",
    "plt.legend([\"Singularly\", \"PI\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8Fto9wzxMoz",
    "outputId": "20360abf-0e98-4693-cf09-152fa15d7d2b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PI_random_forest_top_20_features = np.argsort(PI_random_forest_importance)[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "PI_logistic_regression_top_20_features = np.argsort(PI_logistic_regression_importance)[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "PI_gradient_boosting_top_20_features = np.argsort(PI_gradient_boosting_importance)[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "PI_svm_top_20_features = np.argsort(PI_svm_importance)[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "\n",
    "print(f\"\"\"\n",
    "Accuracies:\n",
    "Random Forest: {(accuracies_filtered_PI[-int(len(accuracies_filtered_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][0]).round(2)}%\n",
    "Logistic Regression: {(accuracies_filtered_PI[-int(len(accuracies_filtered_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][1]).round(2)}%\n",
    "Gradient Boosting: {(accuracies_filtered_PI[-int(len(accuracies_filtered_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][2]).round(2)}%\n",
    "SVC: {(accuracies_filtered_PI[-int(len(accuracies_filtered_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][3]).round(2)}%\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zN4SAuURxMoz",
    "outputId": "edf0fa16-ec13-4880-c3cd-ac0a2779883b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('R F / R F with PI')\n",
    "print(np.array((random_forest_top_20_features, PI_random_forest_top_20_features)).squeeze())\n",
    "print('L R / L R with PI')\n",
    "print(np.array((logistic_regression_top_20_features,PI_logistic_regression_top_20_features)).squeeze())\n",
    "print('G B / G B with PI')\n",
    "print(np.array((gradient_boosting_top_20_features,PI_gradient_boosting_top_20_features)).squeeze())\n",
    "print('SVM / SVM with PI')\n",
    "print(np.array((svm_top_20_features,PI_svm_top_20_features)).squeeze())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### PI with other singular classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9THjd1LxMoy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Features importances plot for each classifier\n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=0).fit(X,y)\n",
    "logistic_regression = LogisticRegression(random_state=0).fit(X,y)\n",
    "gradient_boosting = GradientBoostingClassifier(random_state=0).fit(X,y)\n",
    "svm = SVC(kernel = \"linear\", random_state=0).fit(X,y)\n",
    "knn = KNeighborsClassifier().fit(X,y)\n",
    "cat_nb = CategoricalNB().fit(X,y)\n",
    "line_da = LinearDiscriminantAnalysis().fit(X,y)\n",
    "quad_da = QuadraticDiscriminantAnalysis().fit(X,y)\n",
    "ada_boost = AdaBoostClassifier().fit(X,y)\n",
    "hist_gb = HistGradientBoostingClassifier().fit(X,y)\n",
    "\n",
    "PI_random_forest = permutation_importance(random_forest, X, y, n_repeats=10, random_state=0)\n",
    "PI_logistic_regression = permutation_importance(logistic_regression, X, y, n_repeats=10, random_state=0)\n",
    "PI_gradient_boosting = permutation_importance(gradient_boosting, X, y, n_repeats=10, random_state=0)\n",
    "PI_svm = permutation_importance(svm, X, y, n_repeats=10, random_state=0)\n",
    "PI_knn = permutation_importance(knn, X, y, n_repeats=10, random_state=0)\n",
    "PI_cat_nb = permutation_importance(cat_nb, X, y, n_repeats=10, random_state=0)\n",
    "PI_line_da = permutation_importance(line_da, X, y, n_repeats=10, random_state=0)\n",
    "PI_quad_da = permutation_importance(quad_da, X, y, n_repeats=10, random_state=0)\n",
    "PI_ada_boost = permutation_importance(ada_boost, X, y, n_repeats=10, random_state=0)\n",
    "PI_hist_gb = permutation_importance(hist_gb, X, y, n_repeats=10, random_state=0)\n",
    "\n",
    "PI_random_forest_importance = to_distribution(PI_random_forest.importances_mean)\n",
    "PI_logistic_regression_importance = to_distribution(PI_logistic_regression.importances_mean)\n",
    "PI_gradient_boosting_importance = to_distribution(PI_gradient_boosting.importances_mean)\n",
    "PI_svm_importance = to_distribution(PI_svm.importances_mean)\n",
    "PI_knn_importance = to_distribution(PI_knn.importances_mean)\n",
    "PI_cat_nb_importance = to_distribution(PI_cat_nb.importances_mean)\n",
    "PI_line_da_importance = to_distribution(PI_line_da.importances_mean)\n",
    "PI_quad_da_importance = to_distribution(PI_quad_da.importances_mean)\n",
    "PI_ada_boost_importance = to_distribution(PI_ada_boost.importances_mean)\n",
    "PI_hist_gb_importance = to_distribution(PI_hist_gb.importances_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "cLz6pM6KxMoy",
    "outputId": "716f320e-7ad0-456d-d68c-cbb1f4b253f4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_bar = np.arange(1, X.shape[-1]+1)\n",
    "width = .08\n",
    "plt.figure(figsize=(25,8))\n",
    "plt.title(\"PI feature importances\")\n",
    "plt.bar(x_bar, PI_random_forest_importance, width=width)\n",
    "plt.bar(x_bar+width, PI_logistic_regression_importance, width=width)\n",
    "plt.bar(x_bar+2*width, PI_gradient_boosting_importance, width=width)\n",
    "plt.bar(x_bar+3*width, PI_svm_importance, width=width)\n",
    "plt.bar(x_bar+4*width, PI_knn_importance, width=width)\n",
    "plt.bar(x_bar+5*width, PI_cat_nb_importance, width=width)\n",
    "plt.bar(x_bar+6*width, PI_line_da_importance, width=width)\n",
    "plt.bar(x_bar+7*width, PI_quad_da_importance, width=width)\n",
    "plt.bar(x_bar+8*width, PI_ada_boost_importance, width=width)\n",
    "plt.bar(x_bar+9*width, PI_hist_gb_importance, width=width)\n",
    "plt.legend([\"PI rf\", \"PI lr\", \"PI gb\", \"PI svm\", \"PI knn\", \"PI cat_nb\", \"PI line_da\", \"PI quad_da\", \"PI ada_boost\", \"PI hist_gb\"])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "KLMTOk-KxMoz",
    "outputId": "9568caad-bc48-448a-f750-224ebfa7022f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Accuracies for each classifier \n",
    "\n",
    "accuracies_filtered_PI = []\n",
    "X_filtered_rf = np.copy(X)\n",
    "X_filtered_lr = np.copy(X)\n",
    "X_filtered_gb = np.copy(X)\n",
    "X_filtered_svm = np.copy(X)\n",
    "X_filtered_knn = np.copy(X)\n",
    "X_filtered_cat_nb = np.copy(X)\n",
    "X_filtered_line_da = np.copy(X)\n",
    "X_filtered_quad_da = np.copy(X)\n",
    "X_filtered_ada_boost = np.copy(X)\n",
    "X_filtered_hist_gb = np.copy(X)\n",
    "\n",
    "for i in trange(0, X.shape[-1]):\n",
    "\n",
    "  accuracies_filtered_PI.append([\n",
    "\t\t(cross_val_score(random_forest, X_filtered_rf, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(logistic_regression, X_filtered_lr, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(gradient_boosting, X_filtered_gb, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(svm, X_filtered_svm, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "    (cross_val_score(knn, X_filtered_knn, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "    (cross_val_score(cat_nb, X_filtered_cat_nb, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "    (cross_val_score(line_da, X_filtered_line_da, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "    (cross_val_score(quad_da, X_filtered_quad_da, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "    (cross_val_score(ada_boost, X_filtered_ada_boost, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "    (cross_val_score(hist_gb, X_filtered_hist_gb, y, cv=N_CV, n_jobs=-1).mean()*100).round(2)\n",
    "\t])\n",
    "\n",
    "  random_forest_filtered = RandomForestClassifier().fit(X_filtered_rf,y)\n",
    "  logistic_regression_filtered = LogisticRegression().fit(X_filtered_lr,y)\n",
    "  gradient_boosting_filtered = GradientBoostingClassifier().fit(X_filtered_gb,y)\n",
    "  svm_filtered = SVC(kernel = \"linear\").fit(X_filtered_svm,y)\n",
    "  knn_filtered = KNeighborsClassifier().fit(X_filtered_knn,y)\n",
    "  cat_nb_filtered = CategoricalNB().fit(X_filtered_cat_nb,y)\n",
    "  line_da_filtered = LinearDiscriminantAnalysis().fit(X_filtered_line_da,y)\n",
    "  quad_da_filtered = QuadraticDiscriminantAnalysis().fit(X_filtered_quad_da,y)\n",
    "  ada_boost_filtered = AdaBoostClassifier().fit(X_filtered_ada_boost,y)\n",
    "  hist_gb_filtered = HistGradientBoostingClassifier().fit(X_filtered_hist_gb,y)\n",
    "\n",
    "  PI_random_forest_filtered = permutation_importance(random_forest_filtered, X_filtered_rf, y, n_repeats=10, random_state=0)\n",
    "  PI_logistic_regression_filtered = permutation_importance(logistic_regression_filtered, X_filtered_lr, y, n_repeats=10, random_state=0)\n",
    "  PI_gradient_boosting_filtered = permutation_importance(gradient_boosting_filtered, X_filtered_gb, y, n_repeats=10, random_state=0)\n",
    "  PI_svm_filtered = permutation_importance(svm_filtered, X_filtered_svm, y, n_repeats=10, random_state=0)\n",
    "  PI_knn_filtered = permutation_importance(knn_filtered, X_filtered_knn, y, n_repeats=10, random_state=0)\n",
    "  PI_cat_nb_filtered = permutation_importance(cat_nb_filtered, X_filtered_cat_nb, y, n_repeats=10, random_state=0)\n",
    "  PI_line_da_filtered = permutation_importance(line_da_filtered, X_filtered_line_da, y, n_repeats=10, random_state=0)\n",
    "  PI_quad_da_filtered = permutation_importance(quad_da_filtered, X_filtered_quad_da, y, n_repeats=10, random_state=0)\n",
    "  PI_ada_boost_filtered = permutation_importance(ada_boost_filtered, X_filtered_ada_boost, y, n_repeats=10, random_state=0)\n",
    "  PI_hist_gb_filtered = permutation_importance(hist_gb_filtered, X_filtered_hist_gb, y, n_repeats=10, random_state=0)\n",
    "\n",
    "  PI_random_forest_filtered_feature_importance = to_distribution(PI_random_forest_filtered.importances_mean)\n",
    "  PI_logistic_regression_filtered_feature_importance = to_distribution(PI_logistic_regression_filtered.importances_mean)\n",
    "  PI_gradient_boosting_filtered_feature_importance = to_distribution(PI_gradient_boosting_filtered.importances_mean)\n",
    "  PI_svm_filtered_feature_importance = to_distribution(PI_svm_filtered.importances_mean)\n",
    "  PI_knn_filtered_feature_importance = to_distribution(PI_knn_filtered.importances_mean)\n",
    "  PI_cat_nb_filtered_feature_importance = to_distribution(PI_cat_nb_filtered.importances_mean)\n",
    "  PI_line_da_filtered_feature_importance = to_distribution(PI_line_da_filtered.importances_mean)\n",
    "  PI_quad_da_filtered_feature_importance = to_distribution(PI_quad_da_filtered.importances_mean)\n",
    "  PI_ada_boost_filtered_feature_importance = to_distribution(PI_ada_boost_filtered.importances_mean)\n",
    "  PI_hist_gb_filtered_feature_importance = to_distribution(PI_hist_gb_filtered.importances_mean)\n",
    "\n",
    "  X_filtered_rf = X_filtered_rf[:, np.argsort(PI_random_forest_filtered_feature_importance)[::-1][:-1]]\n",
    "  X_filtered_lr = X_filtered_lr[:, np.argsort(PI_logistic_regression_filtered_feature_importance)[::-1][:-1]]\n",
    "  X_filtered_gb = X_filtered_gb[:, np.argsort(PI_gradient_boosting_filtered_feature_importance)[::-1][:-1]]\n",
    "  X_filtered_svm = X_filtered_svm[:, np.argsort(PI_svm_filtered_feature_importance)[::-1][:-1]]\n",
    "  X_filtered_knn = X_filtered_knn[:, np.argsort(PI_knn_filtered_feature_importance)[::-1][:-1]]\n",
    "  X_filtered_cat_nb = X_filtered_cat_nb[:, np.argsort(PI_cat_nb_filtered_feature_importance)[::-1][:-1]]\n",
    "  X_filtered_line_da = X_filtered_line_da[:, np.argsort(PI_line_da_filtered_feature_importance)[::-1][:-1]]\n",
    "  X_filtered_quad_da = X_filtered_quad_da[:, np.argsort(PI_quad_da_filtered_feature_importance)[::-1][:-1]]\n",
    "  X_filtered_ada_boost = X_filtered_ada_boost[:, np.argsort(PI_ada_boost_filtered_feature_importance)[::-1][:-1]]\n",
    "  X_filtered_hist_gb = X_filtered_hist_gb[:, np.argsort(PI_hist_gb_filtered_feature_importance)[::-1][:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-sgkBh6xMoz",
    "outputId": "be3f6572-ab74-4c63-f25f-6643e7e0d585",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_filtered_PI = np.array(accuracies_filtered_PI)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "for i in range(10):\n",
    "\tplt.plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_PI[:,i][::-1])\n",
    "\n",
    "plt.xticks(np.arange(1, X.shape[-1]+1, 4))\n",
    "plt.legend([\"PI rf\", \"PI lr\", \"PI gb\", \"PI svm\", \"PI knn\", \"PI cat_nb\", \"PI line_da\", \"PI quad_da\", \"PI ada_boost\", \"PI hist_gb\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PI_random_forest_top_20_features = np.argsort(PI_random_forest_importance)[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "PI_logistic_regression_top_20_features = np.argsort(PI_logistic_regression_importance)[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "PI_gradient_boosting_top_20_features = np.argsort(PI_gradient_boosting_importance)[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "PI_svm_top_20_features = np.argsort(PI_svm_importance)[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "PI_knn_top_20_features = np.argsort(PI_knn_importance)[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "PI_cat_nb_top_20_features = np.argsort(PI_cat_nb_importance)[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "PI_line_da_top_20_features = np.argsort(PI_line_da_importance)[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "PI_quad_da_top_20_features = np.argsort(PI_quad_da_importance)[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "PI_ada_boost_top_20_features = np.argsort(PI_ada_boost_importance)[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "PI_hist_gb_top_20_features = np.argsort(PI_hist_gb_importance)[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "\n",
    "print(f\"\"\"\n",
    "Accuracies:\n",
    "Random Forest: {(accuracies_filtered_PI[-int(len(accuracies_filtered_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][0]).round(2)}%\n",
    "Logistic Regression: {(accuracies_filtered_PI[-int(len(accuracies_filtered_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][1]).round(2)}%\n",
    "Gradient Boosting: {(accuracies_filtered_PI[-int(len(accuracies_filtered_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][2]).round(2)}%\n",
    "SVC: {(accuracies_filtered_PI[-int(len(accuracies_filtered_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][3]).round(2)}%\n",
    "knn: {(accuracies_filtered_PI[-int(len(accuracies_filtered_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][4]).round(2)}%\n",
    "cat_nb: {(accuracies_filtered_PI[-int(len(accuracies_filtered_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][5]).round(2)}%\n",
    "line_da: {(accuracies_filtered_PI[-int(len(accuracies_filtered_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][6]).round(2)}%\n",
    "quad_da: {(accuracies_filtered_PI[-int(len(accuracies_filtered_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][7]).round(2)}%\n",
    "ada_boost: {(accuracies_filtered_PI[-int(len(accuracies_filtered_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][8]).round(2)}%\n",
    "hist_gb: {(accuracies_filtered_PI[-int(len(accuracies_filtered_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][9]).round(2)}%\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zN4SAuURxMoz",
    "outputId": "edf0fa16-ec13-4880-c3cd-ac0a2779883b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('r f: ', PI_random_forest_top_20_features)\n",
    "print('l r: ', PI_logistic_regression_top_20_features)\n",
    "print('g b: ', PI_gradient_boosting_top_20_features)\n",
    "print('svm: ', PI_svm_top_20_features)\n",
    "print('knn: ', PI_knn_top_20_features)\n",
    "print('cnb: ', PI_cat_nb_top_20_features)\n",
    "print('lda: ', PI_line_da_top_20_features)\n",
    "print('qda: ', PI_quad_da_top_20_features)\n",
    "print('abc: ', PI_ada_boost_top_20_features)\n",
    "print('hgb: ', PI_hist_gb_top_20_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "JYotLk4XxMoz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### LOCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L63aVHVZxMoz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def LOCO(model, X, y):\n",
    "\tres = []\n",
    "\tbaseline = cross_val_score(model, X, y, n_jobs=-1, cv=N_CV).mean()\n",
    "\tfor i in range(X.shape[-1]):\n",
    "\t\tloco_acc = cross_val_score(model, X[:, np.where(np.arange(0, X.shape[-1]) != i)[0]], y, n_jobs=-1, cv=N_CV).mean()\n",
    "\t\tres.append(loco_acc - baseline)\n",
    "\treturn np.array(res)\n",
    "def LOCO_scores_to_importance(res):\n",
    "\tres = -res\n",
    "\treturn res - np.min(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "LkUEWcWqxMo0",
    "outputId": "ed305790-277e-48d1-af33-b965d096fe23",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "np.random.seed(0)\n",
    "random_forest = RandomForestClassifier(random_state=0)\n",
    "plt.bar(np.arange(0, X.shape[-1]) , LOCO_scores_to_importance(LOCO(random_forest, X, y)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T77eyMEHiqVA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(random_state=0)\n",
    "logistic_regression = LogisticRegression(random_state=0)\n",
    "gradient_boosting = GradientBoostingClassifier(random_state=0)\n",
    "svm = SVC(kernel = \"linear\",random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOFl-dyqxMo0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rf_lofoimportance = LOCO_scores_to_importance(LOCO(random_forest, X, y))\n",
    "lr_lofoimportance = LOCO_scores_to_importance(LOCO(logistic_regression, X, y))\n",
    "gb_lofoimportance = LOCO_scores_to_importance(LOCO(gradient_boosting, X, y))\n",
    "svm_lofoimportance = LOCO_scores_to_importance(LOCO(svm, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65pTbqZnxMo0",
    "outputId": "341cdd88-0e27-4b8d-b9b0-fdec25202fd6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_bar = np.arange(1, X.shape[-1]+1)\n",
    "width = .2\n",
    "_, axs = plt.subplots(2, 2, figsize=(20,10))\n",
    "axs[0][0].set_title(\"Random forest feature selection\")\n",
    "axs[0][0].bar(x_bar,to_distribution(rf_lofoimportance), width=width)\n",
    "random_forest.fit(X,y)\n",
    "axs[0][0].bar(x_bar+width, to_distribution(random_forest.feature_importances_), width=width)\n",
    "axs[0][0].legend([\"LOFO importances\", \"Model importances\"])\n",
    "axs[0][1].set_title(\"Logistic regression feature selection\")\n",
    "axs[0][1].bar(x_bar, to_distribution(lr_lofoimportance), width=width)\n",
    "logistic_regression.fit(X,y)\n",
    "axs[0][1].bar(x_bar+width, to_distribution(logistic_regression.coef_.squeeze()), width=width)\n",
    "axs[0][1].legend([\"LOFO importances\", \"Model importances\"])\n",
    "axs[1][0].set_title(\"Gradient Boosting feature selection\")\n",
    "axs[1][0].bar(x_bar, to_distribution(gb_lofoimportance), width=width)\n",
    "gradient_boosting.fit(X,y)\n",
    "axs[1][0].bar(x_bar+width, to_distribution(gradient_boosting.feature_importances_), width=width)\n",
    "axs[1][0].legend([\"LOFO importances\", \"Model importances\"])\n",
    "axs[1][1].set_title(\"SVM feature selection\")\n",
    "axs[1][1].bar(x_bar, to_distribution(svm_lofoimportance), width=width)\n",
    "svm.fit(X,y)\n",
    "axs[1][1].bar(x_bar+width, to_distribution(svm.coef_.squeeze()), width=width)\n",
    "axs[1][1].legend([\"LOFO importances\", \"Model importances\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "2TVwdfQlxMo0",
    "outputId": "56fe1529-edb0-41e1-c507-8d3b9aa0ba47",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_filtered_LOFO= []\n",
    "X_filtered_rf = np.copy(X)\n",
    "X_filtered_lr=np.copy(X)\n",
    "X_filtered_gb=np.copy(X)\n",
    "X_filtered_svm=np.copy(X)\n",
    "for i in trange(0, X.shape[-1]-1):\n",
    "  accuracies_filtered_LOFO.append([\n",
    "\t\t(cross_val_score(random_forest, X_filtered_rf, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(logistic_regression, X_filtered_lr, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(gradient_boosting, X_filtered_gb, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(svm, X_filtered_svm, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t])\n",
    "  rf_filtered_lofo_importance = LOCO_scores_to_importance(LOCO(random_forest, X_filtered_rf, y))\n",
    "  lr_filtered_lofo_importance = LOCO_scores_to_importance(LOCO(logistic_regression, X_filtered_lr, y))\n",
    "  gb_filtered_lofo_importance = LOCO_scores_to_importance(LOCO(gradient_boosting, X_filtered_gb, y))\n",
    "  svm_filtered_lofo_importance = LOCO_scores_to_importance(LOCO(svm, X_filtered_svm, y))\n",
    "\n",
    "  lofo_random_forest_filtered_feature_importance = to_distribution(rf_filtered_lofo_importance)\n",
    "  lofo_logistic_regression_filtered_feature_importance = to_distribution(lr_filtered_lofo_importance)\n",
    "  lofo_gradient_boosting_filtered_feature_importance = to_distribution(gb_filtered_lofo_importance)\n",
    "  lofo_svm_filtered_feature_importance = to_distribution(svm_filtered_lofo_importance)\n",
    "\n",
    "  X_filtered_rf = X_filtered_rf[:, np.argsort(lofo_random_forest_filtered_feature_importance)[::-1][:-1]]\n",
    "  X_filtered_lr = X_filtered_lr[:, np.argsort(lofo_logistic_regression_filtered_feature_importance)[::-1][:-1]]\n",
    "  X_filtered_gb = X_filtered_gb[:, np.argsort( lofo_gradient_boosting_filtered_feature_importance)[::-1][:-1]]\n",
    "  X_filtered_svm = X_filtered_svm[:, np.argsort(lofo_svm_filtered_feature_importance)[::-1][:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4o1RZB0lxMo0",
    "outputId": "d199d9ef-b18d-4c9e-e5ab-01297bb45feb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_filtered_singularly = np.array(accuracies_filtered_singularly)\n",
    "accuracies_filtered_LOFO = np.array(accuracies_filtered_LOFO)\n",
    "\n",
    "_, axs = plt.subplots(1, 4, figsize=(15,5))\n",
    "for i in range(4):\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_singularly[:,i][::-1])\n",
    "\taxs[i].plot(np.arange(2, X.shape[-1]+1), accuracies_filtered_LOFO[:,i][::-1])\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_PI[:,i][::-1])\n",
    "\taxs[i].set_xticks(np.arange(1, X.shape[-1], 3))\n",
    "plt.legend([\"Singularly\", \"LOFO\", \"PI\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y2jWJLt-xMo0",
    "outputId": "79cd0e82-9a33-4dc7-c2e8-3c186ebd19cb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LOFO_random_forest_top_20_features = np.argsort(rf_lofoimportance)[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "LOFO_logistic_regression_top_20_features = np.argsort(lr_lofoimportance)[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "LOFO_gradient_boosting_top_20_features=np.argsort(gb_lofoimportance)[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "LOFO_svm_top_20_features=np.argsort(svm_lofoimportance)[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "print(f\"\"\"\n",
    "Accuracies:\n",
    "Random Forest: {(accuracies_filtered_LOFO[-int(len(accuracies_filtered_LOFO) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][0]).round(2)}%\n",
    "Logistic Regression: {(accuracies_filtered_LOFO[-int(len(accuracies_filtered_LOFO) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][1]).round(2)}%\n",
    "Gradient Boosting: {(accuracies_filtered_LOFO[-int(len(accuracies_filtered_LOFO) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][2]).round(2)}%\n",
    "SVC: {(accuracies_filtered_LOFO[-int(len(accuracies_filtered_LOFO) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][3]).round(2)}%\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "_gfcQXc8E37i",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## FS with model independent without classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "bW6TXf6exMo0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "here:\n",
    "\t- PCA (allenarlo su tutto vs allenarlo solo su quelli onesti)\n",
    "\t- SparsePCA\n",
    "\t- Factor analysis\n",
    "  \t- Relief\n",
    "  \t- Mutual Information\n",
    "  \t- Minimum Redundancy Maximum Relevance (mRMR)\n",
    "  \t- tutta la roba che c'è qua https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZkXNmGZxMo1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import FactorAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4onUy-QxMo1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calc_loadings(X, X_trans):\n",
    "\tloadings = np.zeros(shape = (X_trans.shape[1], X.shape[1]))\n",
    "\tfor i in range(X_trans.shape[1]):\n",
    "\t\tfor j in range(X.shape[1]):\n",
    "\t\t\tloadings[i,j] = np.cov(X_trans[:,i], X[:,j])[0][1]\n",
    "\treturn loadings\n",
    "def loadings_to_importance(loadings):\n",
    "\treturn np.abs(loadings).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "iywSrbPHxMo1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "41M2969TxMo1",
    "outputId": "de4c2e7f-97d7-4b6e-f4fb-5280c8ac0b63",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# PCA without limits in number of components or explained variance\n",
    "pca = PCA()\n",
    "# Fitting on all the data\n",
    "pca.fit(X)\n",
    "\n",
    "plt.imshow(np.abs(calc_loadings(X, pca.transform(X))))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-3FFU8uxMo1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(random_state=0).fit(X, y)\n",
    "logistic_regression = LogisticRegression(random_state=0).fit(X, y)\n",
    "gradient_boosting = GradientBoostingClassifier(random_state=0).fit(X, y)\n",
    "svm = SVC(kernel=\"linear\", random_state=0).fit(X, y)\n",
    "summed_importances = np.sum([\n",
    "\tto_distribution(random_forest.feature_importances_),\n",
    "\tto_distribution(logistic_regression.coef_.squeeze()),\n",
    "\tto_distribution(gradient_boosting.feature_importances_),\n",
    "\tto_distribution(svm.coef_.squeeze())\n",
    "], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zweeEeiYxMo1",
    "outputId": "ab3bdb1b-7360-4aef-eaa5-4a9bf1d39093",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_bar = np.arange(1, X.shape[-1]+1)\n",
    "width = .2\n",
    "plt.figure(dpi=200, figsize=(20,5))\n",
    "plt.bar(x_bar, to_distribution(loadings_to_importance(calc_loadings(X, pca.transform(X)))), width=width)\n",
    "plt.bar(x_bar+width, to_distribution(summed_importances), width=width)\n",
    "plt.legend([\"PCA importances\", \"summed importances\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgf6yzQmxMo1",
    "outputId": "5ab243b4-d021-472b-b941-0cb4461221be",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_filtered_PCA_BS = []\n",
    "X_filtered = np.copy(X)\n",
    "\n",
    "for i in trange(0, X.shape[-1]):\n",
    "\taccuracies_filtered_PCA_BS.append([\n",
    "\t\t(cross_val_score(random_forest, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(logistic_regression, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(gradient_boosting, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(svm, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t])\n",
    "\tpca = PCA()\n",
    "\tpca.fit(X_filtered)\n",
    "\tloadings = calc_loadings(X_filtered, pca.transform(X_filtered))\n",
    "\timportances = loadings_to_importance(loadings)\n",
    "\n",
    "\tX_filtered = X_filtered[:, np.argsort(to_distribution(importances))[::-1][:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grweY15sxMo1",
    "outputId": "e3b4f954-6919-4ef4-a285-b9f4f4e89551",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_filtered_PCA = []\n",
    "X_filtered = np.copy(X)\n",
    "pca = PCA()\n",
    "pca.fit(X_filtered)\n",
    "loadings = calc_loadings(X_filtered, pca.transform(X_filtered))\n",
    "importances = loadings_to_importance(loadings)\n",
    "\n",
    "for i in trange(0, X.shape[-1]):\n",
    "\taccuracies_filtered_PCA.append([\n",
    "\t\t(cross_val_score(random_forest, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(logistic_regression, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(gradient_boosting, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(svm, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t])\n",
    "\n",
    "\tX_filtered = X[:, np.argsort(to_distribution(importances))[::-1][:-(i+1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqPMLtgdxMo2",
    "outputId": "2b46c884-0ff0-4012-9666-97fd17b51be5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_filtered_singularly = np.array(accuracies_filtered_singularly)\n",
    "accuracies_filtered_PCA_BS = np.array(accuracies_filtered_PCA_BS)\n",
    "accuracies_filtered_PCA = np.array(accuracies_filtered_PCA)\n",
    "\n",
    "_, axs = plt.subplots(1, 4, figsize=(15,5))\n",
    "for i in range(4):\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_singularly[:,i][::-1])\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_PCA[:,i][::-1])\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_PCA_BS[:,i][::-1])\n",
    "\taxs[i].set_xticks(np.arange(1, X.shape[-1], 3))\n",
    "plt.legend([\"Singularly\", \"PCA\", \"PCA BS\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ATDuqLSCxMo2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dR8_2UrAxMo2",
    "outputId": "e47f2af7-d4b9-4ac9-b73d-41cf13aac67d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# KernelPCA without limits in number of components or explained variance\n",
    "kernelpca = KernelPCA(kernel=\"cosine\")\n",
    "# Fitting on all the data\n",
    "kernelpca.fit(X)\n",
    "plt.imshow(np.abs(calc_loadings(X, kernelpca.transform(X))))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pC0-GnFBxMo2",
    "outputId": "32241412-77bc-49eb-e3f5-8813f156f6c8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_filtered_KPCA = []\n",
    "X_filtered = np.copy(X)\n",
    "\n",
    "for i in trange(0, X.shape[-1]):\n",
    "\taccuracies_filtered_KPCA.append([\n",
    "\t\t(cross_val_score(random_forest, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(logistic_regression, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(gradient_boosting, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(svm, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t])\n",
    "\tkpca = KernelPCA(kernel=\"cosine\")\n",
    "\tkpca.fit(X_filtered)\n",
    "\tloadings = calc_loadings(X_filtered, kpca.transform(X_filtered))\n",
    "\timportances = loadings_to_importance(loadings)\n",
    "\n",
    "\tX_filtered = X_filtered[:, np.argsort(to_distribution(importances))[::-1][:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIu50rPlxMo2",
    "outputId": "db20ba1c-5bfc-4be7-b2f0-30a9ac7ccea7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_filtered_KPCA_honest = []\n",
    "X_filtered = np.copy(X)\n",
    "\n",
    "for i in trange(0, X.shape[-1]):\n",
    "\taccuracies_filtered_KPCA_honest.append([\n",
    "\t\t(cross_val_score(random_forest, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(logistic_regression, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(gradient_boosting, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(svm, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t])\n",
    "\tkpca = KernelPCA(kernel=\"cosine\")\n",
    "\tkpca.fit(X_filtered[y==1])\n",
    "\tloadings = calc_loadings(X_filtered[y==1], kpca.transform(X_filtered[y==1]))\n",
    "\timportances = loadings_to_importance(loadings)\n",
    "\n",
    "\tX_filtered = X_filtered[:, np.argsort(to_distribution(importances))[::-1][:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ac1QgaUyxMo2",
    "outputId": "1e28e3bc-e66d-47dc-d460-8961f14cc7ea",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_filtered_KPCA_honest = np.array(accuracies_filtered_KPCA_honest)\n",
    "accuracies_filtered_KPCA = np.array(accuracies_filtered_KPCA)\n",
    "\n",
    "_, axs = plt.subplots(1, 4, figsize=(15,5))\n",
    "for i in range(4):\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_singularly[:,i][::-1])\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_KPCA[:,i][::-1])\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_KPCA_honest[:,i][::-1])\n",
    "\taxs[i].set_xticks(np.arange(1, X.shape[-1], 3))\n",
    "plt.legend([\"Singularly\", \"KPCA\", \"KPCA honest only\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "iwTCTfYlxMo2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sparse PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igZKUNh2xMo2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# SparsePCA without limits in number of components or explained variance\n",
    "sparsepca = SparsePCA()\n",
    "# Fitting on all the data\n",
    "sparsepca.fit(X)\n",
    "plt.imshow(np.abs(calc_loadings(X, sparsepca.transform(X))))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuxnspXzxMo2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_filtered_SparsePCA = []\n",
    "X_filtered = np.copy(X)\n",
    "\n",
    "for i in trange(0, X.shape[-1]):\n",
    "\taccuracies_filtered_SparsePCA.append([\n",
    "\t\t(cross_val_score(random_forest, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(logistic_regression, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(gradient_boosting, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(svm, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t])\n",
    "\tsparse_pca = SparsePCA()\n",
    "\tsparse_pca.fit(X_filtered)\n",
    "\tloadings = calc_loadings(X_filtered, sparse_pca.transform(X_filtered))\n",
    "\timportances = loadings_to_importance(loadings)\n",
    "\n",
    "\tX_filtered = X_filtered[:, np.argsort(to_distribution(importances))[::-1][:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAFaDRJlxMo3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_filtered_SparsePCA = np.array(accuracies_filtered_SparsePCA)\n",
    "\n",
    "_, axs = plt.subplots(1, 4, figsize=(15,5))\n",
    "for i in range(4):\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_singularly[:,i][::-1])\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_KPCA[:,i][::-1])\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_PCA_BS[:,i][::-1])\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_SparsePCA[:,i][::-1])\n",
    "\taxs[i].set_xticks(np.arange(1, X.shape[-1], 3))\n",
    "plt.legend([\"Singularly\", \"KPCA\", \"PCA BS\", \"SparsePCA\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "EkPXv_sPxMo3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IbrVDqCxMo3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TruncatedSVD without limits in number of components or explained variance\n",
    "truncatedsvd = TruncatedSVD(n_components = X.shape[1]-1)\n",
    "# Fitting on all the data\n",
    "truncatedsvd.fit(X)\n",
    "plt.imshow(np.abs(calc_loadings(X, truncatedsvd.transform(X))))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRc6dm89xMo3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_filtered_trunc_SVD = []\n",
    "X_filtered = np.copy(X)\n",
    "\n",
    "for i in trange(0, X.shape[-1]-1):\n",
    "\taccuracies_filtered_trunc_SVD.append([\n",
    "\t\t(cross_val_score(random_forest, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(logistic_regression, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(gradient_boosting, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(svm, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t])\n",
    "\ttruncated_svd = TruncatedSVD()\n",
    "\ttruncated_svd.fit(X_filtered)\n",
    "\tloadings = calc_loadings(X_filtered, truncated_svd.transform(X_filtered))\n",
    "\timportances = loadings_to_importance(loadings)\n",
    "\n",
    "\tX_filtered = X_filtered[:, np.argsort(to_distribution(importances))[::-1][:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5Ko85PfxMo3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_filtered_trunc_SVD = np.array(accuracies_filtered_trunc_SVD)\n",
    "\n",
    "_, axs = plt.subplots(1, 4, figsize=(15,5))\n",
    "for i in range(4):\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_singularly[:,i][::-1])\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_KPCA[:,i][::-1])\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_PCA_BS[:,i][::-1])\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_SparsePCA[:,i][::-1])\n",
    "\taxs[i].plot(np.arange(2, X.shape[-1]+1), accuracies_filtered_trunc_SVD[:,i][::-1])\n",
    "\taxs[i].set_xticks(np.arange(1, X.shape[-1], 3))\n",
    "plt.legend([\"Singularly\", \"KPCA\", \"PCA BS\", \"SparsePCA\", \"TruncatedSVD\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "GzToyqJvxMo3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Factor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4euG_8sxMo3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# FastICA without limits in number of components or explained variance\n",
    "factor = FactorAnalysis()\n",
    "# Fitting on all the data\n",
    "factor.fit(X)\n",
    "plt.imshow(np.abs(calc_loadings(X, factor.transform(X))))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJIcU-nQxMo3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_filtered_factor_analysis= []\n",
    "X_filtered = np.copy(X)\n",
    "\n",
    "for i in trange(0, X.shape[-1]):\n",
    "\taccuracies_filtered_factor_analysis.append([\n",
    "\t\t(cross_val_score(random_forest, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(logistic_regression, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(gradient_boosting, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(svm, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t])\n",
    "\tfactor_analysis = FactorAnalysis()\n",
    "\tfactor_analysis.fit(X_filtered)\n",
    "\tloadings = calc_loadings(X_filtered, factor_analysis.transform(X_filtered))\n",
    "\timportances = loadings_to_importance(loadings)\n",
    "\n",
    "\tX_filtered = X_filtered[:, np.argsort(to_distribution(importances))[::-1][:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWMLrPZixMo3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_filtered_factor_analysis = np.array(accuracies_filtered_factor_analysis)\n",
    "\n",
    "_, axs = plt.subplots(1, 4, figsize=(15,5))\n",
    "for i in range(4):\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_singularly[:,i][::-1])\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_KPCA[:,i][::-1])\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_PCA_BS[:,i][::-1])\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_SparsePCA[:,i][::-1])\n",
    "\taxs[i].plot(np.arange(2, X.shape[-1]+1), accuracies_filtered_trunc_SVD[:,i][::-1])\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_factor_analysis[:,i][::-1])\n",
    "\taxs[i].set_xticks(np.arange(1, X.shape[-1], 3))\n",
    "plt.legend([\"Singularly\", \"KPCA\", \"PCA BS\", \"SparsePCA\", \"TruncatedSVD\", \"Factor Analysis\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Em_4Y4vUxMo3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Relief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1n6Oxu_QyUYC",
    "outputId": "547a21b6-0e1c-40e9-df64-e414c045979e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install skrebate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EeJN8ycyB4X",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from skrebate import ReliefF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDhvGT6dxMo3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "relief = ReliefF(n_neighbors=5).fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYE3TphzxMo4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(random_state=0).fit(X,y)\n",
    "logistic_regression = LogisticRegression(random_state=0).fit(X,y)\n",
    "gradient_boosting = GradientBoostingClassifier(random_state=0).fit(X,y)\n",
    "svm = SVC(kernel = \"linear\", random_state=0).fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gc44qyG-xMo4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "summed_importances=np.sum([\n",
    "\tto_distribution(random_forest.feature_importances_),\n",
    "\tto_distribution(logistic_regression.coef_.squeeze()),\n",
    "\tto_distribution(gradient_boosting.feature_importances_),\n",
    "\tto_distribution( svm.coef_.squeeze())\n",
    "], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vl7lvhT7xMo4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_bar = np.arange(1, X.shape[-1]+1)\n",
    "width = .2\n",
    "plt.figure(dpi=200, figsize=(20,5))\n",
    "plt.bar(x_bar, to_distribution(relief.feature_importances_), width=width)\n",
    "plt.bar(x_bar+width, to_distribution(summed_importances), width=width)\n",
    "plt.legend([\"Relief importances\", \"Summed importances\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qD--CP0qxMo4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_with_relief_selection= []\n",
    "X_filtered = np.copy(X)\n",
    "for i in trange(0, X.shape[-1]):\n",
    "\taccuracies_with_relief_selection.append([\n",
    "\t\t(cross_val_score(random_forest, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(logistic_regression, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(gradient_boosting, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(svm, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t])\n",
    "\n",
    "\trelief = ReliefF(n_neighbors=5).fit(X_filtered,y)\n",
    "\trelief_feature_importance=to_distribution(relief.feature_importances_)\n",
    "\tX_filtered= X_filtered[:, np.argsort(relief_feature_importance)[::-1][:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQBmqhKUxMo4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "axs[0].set_title(\"Individual feature selection\")\n",
    "axs[0].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_singularly[::-1])\n",
    "axs[0].set_xticks(np.arange(1, X.shape[-1]+1, 3))\n",
    "axs[0].legend([\"Random forest\", \"Logistic regression\", \"Gradient boosting\", \"SVM\"])\n",
    "axs[0].set_xlabel(\"Number of top feature considered in the training\")\n",
    "axs[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "axs[1].set_title(\"Relief feature selection\")\n",
    "axs[1].plot(np.arange(1, X.shape[-1]+1), accuracies_with_relief_selection[::-1])\n",
    "axs[1].set_xticks(np.arange(1, X.shape[-1]+1, 3))\n",
    "axs[1].legend([\"Random forest\", \"Logistic regression\", \"Gradient boosting\", \"SVM\"])\n",
    "axs[1].set_xlabel(\"Number of top feature considered in the training\")\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CoqrRPvMxMo4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_filtered_singularly = np.array(accuracies_filtered_singularly)\n",
    "accuracies_with_relief_selection = np.array(accuracies_with_relief_selection)\n",
    "\n",
    "_, axs = plt.subplots(1, 4, figsize=(15,5))\n",
    "for i in range(4):\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_singularly[:,i][::-1])\n",
    "\taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_with_relief_selection[:,i][::-1])\n",
    "\taxs[i].set_xticks(np.arange(1, X.shape[-1]+1, 3))\n",
    "plt.legend([\"Singularly\", \"Relief\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19dtXnbVxMo4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "relief_top_20_features = np.argsort(to_distribution(relief.feature_importances_))[::-1][:int(original_dataset_size * (1-PERCENTAGE_FEATURE_TO_REMOVE))]\n",
    "\n",
    "print(f\"\"\"\n",
    "Accuracies:\n",
    "Random Forest: {(accuracies_with_relief_selection[-int(len(accuracies_with_relief_selection) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][0]).round(2)}%\n",
    "Logistic Regression: {(accuracies_with_relief_selection[-int(len(accuracies_with_relief_selection) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][1]).round(2)}%\n",
    "Gradient Boosting: {(accuracies_with_relief_selection[-int(len(accuracies_with_relief_selection) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][2]).round(2)}%\n",
    "SVC: {(accuracies_with_relief_selection[-int(len(accuracies_with_relief_selection) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][3]).round(2)}%\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVVJGVKtx5jX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Robustness of methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Robustness Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "def leaderboards_to_importance(elms):\n",
    "\telms = np.array([\n",
    "\t\tl.argsort()\n",
    "\t\tfor l in elms\n",
    "\t])\n",
    "\treturn np.mean([\n",
    "\t\tnp.linalg.norm(elms[c[0]] - elms[c[1]],ord=1)\n",
    "\t\tfor c in combinations(list(range(len(elms))), 2)\n",
    "\t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "example = np.array([\n",
    "\t[4,2,3,1,0],\n",
    "\t[4,3,1,2,0],\n",
    "\t[4,3,2,1,0],\n",
    "\t[4,3,2,1,0]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "leaderboards_to_importance(example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "tz7vBP51x0V1",
    "outputId": "307b9ad5-8698-43b4-c901-4a0a57f7eb5c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_filtered_together_PI = []\n",
    "X_filtered = np.copy(X)\n",
    "feature_importance_PI = []\n",
    "features_selected_at_each_step_PI = [np.arange(0, X.shape[-1]).tolist()]\n",
    "features_importance_at_each_step_PI = []\n",
    "for i in trange(X.shape[-1], 0, -1):\n",
    "\taccuracies_filtered_together_PI.append([\n",
    "\t\t# (cross_val_score(random_forest, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(logistic_regression, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t# (cross_val_score(gradient_boosting, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(svm, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t# (cross_val_score(knn, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(cat_nb, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(line_da, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t# (cross_val_score(quad_da, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t# (cross_val_score(ada_boost, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(hist_gb, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2)\n",
    "\t])\n",
    "\tif i > 1 :\n",
    "\t\t# random_forest=RandomForestClassifier(random_state=0).fit(X_filtered, y)\n",
    "\t\tlogistic_regression=LogisticRegression(random_state=0).fit(X_filtered, y)\n",
    "\t\t# gradient_boosting=GradientBoostingClassifier(random_state=0).fit(X_filtered, y)\n",
    "\t\tsvm = SVC(kernel = \"linear\",random_state=0).fit(X_filtered, y)\n",
    "\t\t# knn = KNeighborsClassifier().fit(X_filtered,y)\n",
    "\t\tcat_nb = CategoricalNB().fit(X_filtered,y)\n",
    "\t\tline_da = LinearDiscriminantAnalysis().fit(X_filtered,y)\n",
    "\t\t# quad_da = QuadraticDiscriminantAnalysis().fit(X_filtered,y)\n",
    "\t\t# ada_boost = AdaBoostClassifier().fit(X_filtered,y)\n",
    "\t\thist_gb = HistGradientBoostingClassifier().fit(X_filtered,y)\n",
    "\n",
    "\t\t# random_forest_feature_importance_PI = to_distribution(permutation_importance(random_forest, X_filtered, y, n_repeats=10, random_state=0).importances_mean)\n",
    "\t\tlogistic_regression_feature_importance_PI = to_distribution(permutation_importance(logistic_regression, X_filtered, y, n_repeats=10, random_state=0).importances_mean)\n",
    "\t\t# gradient_boosting_feature_importance_PI = to_distribution(permutation_importance(gradient_boosting, X_filtered, y, n_repeats=10, random_state=0).importances_mean)\n",
    "\t\tsvm_feature_importance_PI = to_distribution(permutation_importance(svm, X_filtered, y, n_repeats=10, random_state=0).importances_mean)\n",
    "\t\t# knn_feature_importance_PI = to_distribution(permutation_importance(knn, X_filtered, y, n_repeats=10, random_state=0).importances_mean)\n",
    "\t\tcat_nb_feature_importance_PI = to_distribution(permutation_importance(cat_nb, X_filtered, y, n_repeats=10, random_state=0).importances_mean)\n",
    "\t\tline_da_feature_importance_PI = to_distribution(permutation_importance(line_da, X_filtered, y, n_repeats=10, random_state=0).importances_mean)\n",
    "\t\t# quad_da_feature_importance_PI = to_distribution(permutation_importance(quad_da, X_filtered, y, n_repeats=10, random_state=0).importances_mean)\n",
    "\t\t# ada_boost_feature_importance_PI = to_distribution(permutation_importance(ada_boost, X_filtered, y, n_repeats=10, random_state=0).importances_mean)\n",
    "\t\thist_gb_feature_importance_PI = to_distribution(permutation_importance(hist_gb, X_filtered, y, n_repeats=10, random_state=0).importances_mean)\n",
    "\n",
    "\t\tsummed_importance_PI = np.sum([\n",
    "\t\t\t# random_forest_feature_importance_PI,\n",
    "\t\t\tlogistic_regression_feature_importance_PI,\n",
    "\t\t\t# gradient_boosting_feature_importance_PI,\n",
    "\t\t\tsvm_feature_importance_PI,\n",
    "\t\t\t# knn_feature_importance_PI,\n",
    "\t\t\tcat_nb_feature_importance_PI,\n",
    "\t\t\tline_da_feature_importance_PI,\n",
    "\t\t\t# quad_da_feature_importance_PI,\n",
    "\t\t\t# ada_boost_feature_importance_PI,\n",
    "\t\t\thist_gb_feature_importance_PI\n",
    "\t\t], axis=0)\n",
    "\n",
    "\t\tfeature_importance_PI.append(summed_importance_PI)\n",
    "\t\tselected_features_PI = np.argsort(summed_importance_PI)[::-1][:-1]\n",
    "\t\tfeatures_selected_at_each_step_PI.append(selected_features_PI)\n",
    "\t\tfeatures_importance_at_each_step_PI.append([\n",
    "\t\t\t# random_forest_feature_importance_PI,\n",
    "\t\t\tlogistic_regression_feature_importance_PI,\n",
    "\t\t\t# gradient_boosting_feature_importance_PI,\n",
    "\t\t\tsvm_feature_importance_PI,\n",
    "\t\t\t# knn_feature_importance_PI,\n",
    "\t\t\tcat_nb_feature_importance_PI,\n",
    "\t\t\tline_da_feature_importance_PI,\n",
    "\t\t\t# quad_da_feature_importance_PI,\n",
    "\t\t\t# ada_boost_feature_importance_PI,\n",
    "\t\t\thist_gb_feature_importance_PI\n",
    "\t\t])\n",
    "\n",
    "\t\tX_filtered = X_filtered[:,selected_features_PI]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "NsCxP_ReTSAY",
    "outputId": "d6277deb-525d-4534-f73a-dc737799a120",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_together_PI[::-1])\n",
    "# plt.xticks(np.arange(1, X.shape[-1]+1, 4))\n",
    "\n",
    "# plt.legend([\"Random Forest\",\n",
    "# \"Logistic Regression\",\n",
    "# \"Gradient Boosting\",\n",
    "# \"SVM\",\n",
    "# \"k Nearest Neighbors\",\n",
    "# \"Nayve Bayes\",\n",
    "# \"Linear Discriminant Analysis\",\n",
    "# \"Quadratic Discriminnt Analysis\",\n",
    "# \"AdaBoost\",\n",
    "# \"histogram-based Gradient Boosting\"])\n",
    "\n",
    "# plt.xlabel(\"Number of top feature considered in the training\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "bURGT9gSbG6W",
    "outputId": "6c2e217b-94ee-4879-db2f-fd444a9a3486",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(f\"\"\"\n",
    "# Accuracies:\n",
    "# Random Forest: {(accuracies_filtered_together_PI[-int(len(accuracies_filtered_together_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][0]).round(2)}%\n",
    "# Logistic Regression: {(accuracies_filtered_together_PI[-int(len(accuracies_filtered_together_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][1]).round(2)}%\n",
    "# Gradient Boosting: {(accuracies_filtered_together_PI[-int(len(accuracies_filtered_together_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][2]).round(2)}%\n",
    "# SVM: {(accuracies_filtered_together_PI[-int(len(accuracies_filtered_together_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][3]).round(2)}%\n",
    "# k Nearest Neighbors: {(accuracies_filtered_together_PI[-int(len(accuracies_filtered_together_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][4]).round(2)}%\n",
    "# Nayve Bayes: {(accuracies_filtered_together_PI[-int(len(accuracies_filtered_together_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][5]).round(2)}%\n",
    "# Linear Discriminant Analysis: {(accuracies_filtered_together_PI[-int(len(accuracies_filtered_together_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][6]).round(2)}%\n",
    "# Quadratic Discriminnt Analysis: {(accuracies_filtered_together_PI[-int(len(accuracies_filtered_together_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][7]).round(2)}%\n",
    "# AdaBoost: {(accuracies_filtered_together_PI[-int(len(accuracies_filtered_together_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][8]).round(2)}%\n",
    "# histogram-based Gradient Boosting: {(accuracies_filtered_together_PI[-int(len(accuracies_filtered_together_PI) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][9]).round(2)}%\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "indexes = np.arange(0, X.shape[-1])\n",
    "for i in range(int(np.round(int(X.shape[-1] - np.round(original_dataset_size - original_dataset_size * PERCENTAGE_FEATURE_TO_REMOVE))) + 1)):\n",
    "\tindexes = indexes[features_selected_at_each_step_PI[i]]\n",
    "new_X_PI = X[:,indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# random_forest = RandomForestClassifier(random_state=0).fit(new_X_PI, y)\n",
    "logistic_regression = LogisticRegression(random_state=0).fit(new_X_PI, y)\n",
    "# gradient_boosting = GradientBoostingClassifier(random_state=0).fit(new_X_PI, y)\n",
    "svm = SVC(kernel = \"linear\",random_state=0).fit(new_X_PI, y)\n",
    "# knn = KNeighborsClassifier().fit(new_X_PI,y)\n",
    "cat_nb = CategoricalNB().fit(new_X_PI,y)\n",
    "line_da = LinearDiscriminantAnalysis().fit(new_X_PI,y)\n",
    "# quad_da = QuadraticDiscriminantAnalysis().fit(new_X_PI,y)\n",
    "# ada_boost = AdaBoostClassifier().fit(new_X_PI,y)\n",
    "hist_gb = HistGradientBoostingClassifier().fit(new_X_PI,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# importances for each model for the best features\n",
    "# PI_random_forest_importances = np.argsort(permutation_importance(random_forest, new_X_PI, y, n_repeats=10, random_state=0).importances_mean)\n",
    "PI_logistic_regression_importances = np.argsort(permutation_importance(logistic_regression, new_X_PI, y, n_repeats=10, random_state=0).importances_mean)\n",
    "# PI_gradient_boosting_importances = np.argsort(permutation_importance(gradient_boosting, new_X_PI, y, n_repeats=10, random_state=0).importances_mean)\n",
    "PI_svm_importances = np.argsort(permutation_importance(svm, new_X_PI, y, n_repeats=10, random_state=0).importances_mean)\n",
    "# PI_knn_importances = np.argsort(permutation_importance(knn, new_X_PI, y, n_repeats=10, random_state=0).importances_mean)\n",
    "PI_cat_nb_importances = np.argsort(permutation_importance(cat_nb, new_X_PI, y, n_repeats=10, random_state=0).importances_mean)\n",
    "PI_line_da_importances = np.argsort(permutation_importance(line_da, new_X_PI, y, n_repeats=10, random_state=0).importances_mean)\n",
    "# PI_quad_da_importances = np.argsort(permutation_importance(quad_da, new_X_PI, y, n_repeats=10, random_state=0).importances_mean)\n",
    "# PI_ada_boost_importances = np.argsort(permutation_importance(ada_boost, new_X_PI, y, n_repeats=10, random_state=0).importances_mean)\n",
    "PI_hist_gb_importances = np.argsort(permutation_importance(hist_gb, new_X_PI, y, n_repeats=10, random_state=0).importances_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# soliti 4 classifiers\n",
    "# array([[2, 4, 3, 1, 0],\n",
    "#        [4, 2, 3, 1, 0],\n",
    "#        [3, 4, 1, 2, 0],\n",
    "#        [4, 2, 3, 1, 0]])\n",
    "\n",
    "# tutti 10 (no knn)\n",
    "# array([[4, 2, 3, 1, 0],\n",
    "#        [4, 2, 3, 1, 0],\n",
    "#        [3, 4, 1, 2, 0],\n",
    "#        [4, 2, 3, 1, 0],\n",
    "#        [4, 2, 3, 1, 0],\n",
    "#        [4, 2, 3, 1, 0],\n",
    "#        [3, 4, 1, 2, 0],\n",
    "#        [2, 4, 3, 0, 1],\n",
    "#        [4, 2, 1, 3, 0]])\n",
    "\n",
    "# tutti 10 (no knn, gradient_boosting, quad_da, ada_boost)\n",
    "# array([[3, 4, 2, 1, 0],\n",
    "#        [4, 3, 2, 1, 0],\n",
    "#        [4, 3, 2, 1, 0],\n",
    "#        [4, 3, 2, 1, 0],\n",
    "#        [4, 3, 2, 1, 0],\n",
    "#        [4, 3, 2, 1, 0]])\n",
    "\n",
    "# tutti 10 (no knn, gradient_boosting, quad_da, ada_boost, random_forest)\n",
    "# array([[4, 3, 2, 1, 0],\n",
    "#        [4, 3, 2, 1, 0],\n",
    "#        [4, 3, 2, 1, 0],\n",
    "#        [4, 3, 2, 1, 0],\n",
    "#        [4, 3, 2, 1, 0]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# soliti 4 classifiers\n",
    "# r i = 3.666\n",
    "# acc = 74.072\n",
    "\n",
    "# tutti 10 (no knn)\n",
    "# r i = 3.444\n",
    "# acc = 74.668\n",
    "\n",
    "# tutti 10 (no knn, gradient_boosting, quad_da, ada_boost)\n",
    "# r i = 0.667\n",
    "# acc = 73.623\n",
    "\n",
    "# tutti 10 (no knn, gradient_boosting, quad_da, ada_boost, random_forest)\n",
    "# r i = 0.000\n",
    "# acc = 73.532"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PI_lead = np.array([\n",
    "\t# PI_random_forest_importances,\n",
    "\tPI_logistic_regression_importances,\n",
    "\t# PI_gradient_boosting_importances,\n",
    "\tPI_svm_importances,\n",
    "\t# PI_knn_importances,\n",
    "\tPI_cat_nb_importances,\n",
    "\tPI_line_da_importances,\n",
    "\t# PI_quad_da_importances,\n",
    "\t# PI_ada_boost_importances,\n",
    "\tPI_hist_gb_importances\n",
    "])\n",
    "PI_lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# robustness index\n",
    "PI_robustness_score = leaderboards_to_importance(PI_lead)\n",
    "PI_robustness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#  PI accuracy\n",
    "index = int(X.shape[-1] - np.round(original_dataset_size - original_dataset_size * PERCENTAGE_FEATURE_TO_REMOVE))\n",
    "PI_accuracy = np.array(accuracies_filtered_together_PI)[index].mean()\n",
    "PI_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzUSjOBTeM5f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### LOCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies_filtered_together_LOCO = []\n",
    "X_filtered = np.copy(X)\n",
    "feature_importance_LOCO = []\n",
    "features_selected_at_each_step_LOCO= [np.arange(0, X.shape[-1]).tolist()]\n",
    "features_importance_at_each_step_LOCO = []\n",
    "for i in trange(X.shape[-1], 0, -1):\n",
    "\taccuracies_filtered_together_LOCO.append([\n",
    "\t\t# (cross_val_score(random_forest, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(logistic_regression, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t# (cross_val_score(gradient_boosting, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(svm, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t# (cross_val_score(knn, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(cat_nb, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(line_da, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t# (cross_val_score(quad_da, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t# (cross_val_score(ada_boost, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2),\n",
    "\t\t(cross_val_score(hist_gb, X_filtered, y, cv=N_CV, n_jobs=-1).mean()*100).round(2)\n",
    "\t])\n",
    "\tif i > 1 :\n",
    "\t\t# random_forest=RandomForestClassifier(random_state=0).fit(X_filtered, y)\n",
    "\t\tlogistic_regression=LogisticRegression(random_state=0).fit(X_filtered, y)\n",
    "\t\t# gradient_boosting=GradientBoostingClassifier(random_state=0).fit(X_filtered, y)\n",
    "\t\tsvm = SVC(kernel = \"linear\",random_state=0).fit(X_filtered, y)\n",
    "\t\t# knn = KNeighborsClassifier().fit(X_filtered,y)\n",
    "\t\tcat_nb = CategoricalNB().fit(X_filtered,y)\n",
    "\t\tline_da = LinearDiscriminantAnalysis().fit(X_filtered,y)\n",
    "\t\t# quad_da = QuadraticDiscriminantAnalysis().fit(X_filtered,y)\n",
    "\t\t# ada_boost = AdaBoostClassifier().fit(X_filtered,y)\n",
    "\t\thist_gb = HistGradientBoostingClassifier().fit(X_filtered,y)\n",
    "\n",
    "\t\t# random_forest_feature_importance_LOCO = to_distribution(permutation_importance(random_forest, X_filtered, y, n_repeats=10, random_state=0).importances_mean)\n",
    "\t\tlogistic_regression_feature_importance_LOCO = to_distribution(permutation_importance(logistic_regression, X_filtered, y, n_repeats=10, random_state=0).importances_mean)\n",
    "\t\t# gradient_boosting_feature_importance_LOCO = to_distribution(permutation_importance(gradient_boosting, X_filtered, y, n_repeats=10, random_state=0).importances_mean)\n",
    "\t\tsvm_feature_importance_LOCO = to_distribution(permutation_importance(svm, X_filtered, y, n_repeats=10, random_state=0).importances_mean)\n",
    "\t\t# knn_feature_importance_LOCO = to_distribution(permutation_importance(knn, X_filtered, y, n_repeats=10, random_state=0).importances_mean)\n",
    "\t\tcat_nb_feature_importance_LOCO = to_distribution(permutation_importance(cat_nb, X_filtered, y, n_repeats=10, random_state=0).importances_mean)\n",
    "\t\tline_da_feature_importance_LOCO = to_distribution(permutation_importance(line_da, X_filtered, y, n_repeats=10, random_state=0).importances_mean)\n",
    "\t\t# quad_da_feature_importance_LOCO = to_distribution(permutation_importance(quad_da, X_filtered, y, n_repeats=10, random_state=0).importances_mean)\n",
    "\t\t# ada_boost_feature_importance_LOCO = to_distribution(permutation_importance(ada_boost, X_filtered, y, n_repeats=10, random_state=0).importances_mean)\n",
    "\t\thist_gb_feature_importance_LOCO = to_distribution(permutation_importance(hist_gb, X_filtered, y, n_repeats=10, random_state=0).importances_mean)\n",
    "\n",
    "\t\tsummed_importance_LOCO = np.sum([\n",
    "\t\t\t# random_forest_feature_importance_LOCO,\n",
    "\t\t\tlogistic_regression_feature_importance_LOCO,\n",
    "\t\t\t# gradient_boosting_feature_importance_LOCO,\n",
    "\t\t\tsvm_feature_importance_LOCO,\n",
    "\t\t\t# knn_feature_importance_LOCO,\n",
    "\t\t\tcat_nb_feature_importance_LOCO,\n",
    "\t\t\tline_da_feature_importance_LOCO,\n",
    "\t\t\t# quad_da_feature_importance_LOCO,\n",
    "\t\t\t# ada_boost_feature_importance_LOCO,\n",
    "\t\t\thist_gb_feature_importance_LOCO\n",
    "\t\t], axis=0)\n",
    "\n",
    "\t\tfeature_importance_LOCO.append(summed_importance_LOCO)\n",
    "\t\tselected_features_LOCO = np.argsort(summed_importance_LOCO)[::-1][:-1]\n",
    "\t\tfeatures_selected_at_each_step_LOCO.append(selected_features_LOCO)\n",
    "\t\tfeatures_importance_at_each_step_LOCO.append([\n",
    "\t\t\t# random_forest_feature_importance_LOCO,\n",
    "\t\t\tlogistic_regression_feature_importance_LOCO,\n",
    "\t\t\t# gradient_boosting_feature_importance_LOCO,\n",
    "\t\t\tsvm_feature_importance_LOCO,\n",
    "\t\t\t# knn_feature_importance_LOCO,\n",
    "\t\t\tcat_nb_feature_importance_LOCO,\n",
    "\t\t\tline_da_feature_importance_LOCO,\n",
    "\t\t\t# quad_da_feature_importance_LOCO,\n",
    "\t\t\t# ada_boost_feature_importance_LOCO,\n",
    "\t\t\thist_gb_feature_importance_LOCO\n",
    "\t\t])\n",
    "\n",
    "\t\tX_filtered = X_filtered[:,selected_features_LOCO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "JYM5fzqZekyA",
    "outputId": "4cb0b3ff-ba20-4e63-baa0-bef0b801d230",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_together_LOCO[::-1])\n",
    "# plt.xticks(np.arange(1, X.shape[-1]+1, 3))\n",
    "# plt.legend([\"Random forest\", \"Logistic regression\", \"Gradient boosting\", \"SVM\"])\n",
    "# plt.xlabel(\"Number of top feature considered in the training\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "0MM3lmC9hK1e",
    "outputId": "50814718-fa7f-45dc-f434-6a9557354c46",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# accuracies_filtered_LOFO= np.array(accuracies_filtered_LOFO)\n",
    "# accuracies_filtered_together_LOCO=np.array(accuracies_filtered_together_LOCO)\n",
    "# _, axs = plt.subplots(1, 4, figsize=(15,5))\n",
    "# for i in range(4):\n",
    "# \taxs[i].plot(np.arange(1, X.shape[-1]+1), accuracies_filtered_together_LOCO[:,i][::-1])\n",
    "# \taxs[i].plot(np.arange(2, X.shape[-1]+1), accuracies_filtered_LOFO[:,i][::-1])\n",
    "# \taxs[i].set_xticks(np.arange(1, X.shape[-1]+1, 3))\n",
    "# plt.legend([\"Together\", \"Singularly\"])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "indexes = np.arange(0, X.shape[-1])\n",
    "for i in range(int(np.round(int(X.shape[-1] - np.round(original_dataset_size - original_dataset_size * PERCENTAGE_FEATURE_TO_REMOVE))) + 1)):\n",
    "\tindexes = indexes[features_selected_at_each_step_LOCO[i]]\n",
    "new_X_LOCO = X[:,indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSapsBG6jXfe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# random_forest = RandomForestClassifier(random_state=0).fit(new_X_LOCO, y)\n",
    "logistic_regression = LogisticRegression(random_state=0).fit(new_X_LOCO, y)\n",
    "# gradient_boosting = GradientBoostingClassifier(random_state=0).fit(new_X_LOCO, y)\n",
    "svm = SVC(kernel = \"linear\",random_state=0).fit(new_X_LOCO, y)\n",
    "# knn = KNeighborsClassifier().fit(new_X_LOCO,y)\n",
    "cat_nb = CategoricalNB().fit(new_X_LOCO,y)\n",
    "line_da = LinearDiscriminantAnalysis().fit(new_X_LOCO,y)\n",
    "# quad_da = QuadraticDiscriminantAnalysis().fit(new_X_LOCO,y)\n",
    "# ada_boost = AdaBoostClassifier().fit(new_X_LOCO,y)\n",
    "hist_gb = HistGradientBoostingClassifier().fit(new_X_LOCO,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YENkEvNmiNdt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# importances for each model for the best features\n",
    "# LOCO_random_forest_importances = np.argsort(permutation_importance(random_forest, new_X_LOCO, y, n_repeats=10, random_state=0).importances_mean)\n",
    "LOCO_logistic_regression_importances = np.argsort(permutation_importance(logistic_regression, new_X_LOCO, y, n_repeats=10, random_state=0).importances_mean)\n",
    "# LOCO_gradient_boosting_importances = np.argsort(permutation_importance(gradient_boosting, new_X_LOCO, y, n_repeats=10, random_state=0).importances_mean)\n",
    "LOCO_svm_importances = np.argsort(permutation_importance(svm, new_X_LOCO, y, n_repeats=10, random_state=0).importances_mean)\n",
    "# LOCO_knn_importances = np.argsort(permutation_importance(knn, new_X_LOCO, y, n_repeats=10, random_state=0).importances_mean)\n",
    "LOCO_cat_nb_importances = np.argsort(permutation_importance(cat_nb, new_X_LOCO, y, n_repeats=10, random_state=0).importances_mean)\n",
    "LOCO_line_da_importances = np.argsort(permutation_importance(line_da, new_X_LOCO, y, n_repeats=10, random_state=0).importances_mean)\n",
    "# LOCO_quad_da_importances = np.argsort(permutation_importance(quad_da, new_X_LOCO, y, n_repeats=10, random_state=0).importances_mean)\n",
    "# LOCO_ada_boost_importances = np.argsort(permutation_importance(ada_boost, new_X_LOCO, y, n_repeats=10, random_state=0).importances_mean)\n",
    "LOCO_hist_gb_importances = np.argsort(permutation_importance(hist_gb, new_X_LOCO, y, n_repeats=10, random_state=0).importances_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# soliti 4 classifiers\n",
    "# array([[4, 2, 3, 1, 0],\n",
    "#        [4, 2, 3, 1, 0],\n",
    "#        [3, 4, 1, 2, 0],\n",
    "#        [4, 2, 3, 1, 0]]\n",
    "\n",
    "# tutti 10\n",
    "# array([[4, 3, 2, 1, 0],\n",
    "#        [4, 3, 2, 1, 0],\n",
    "#        [2, 4, 1, 3, 0],\n",
    "#        [4, 3, 2, 1, 0],\n",
    "#        [4, 3, 2, 1, 0],\n",
    "#        [4, 3, 2, 1, 0],\n",
    "#        [4, 3, 2, 1, 0],\n",
    "#        [2, 4, 1, 3, 0],\n",
    "#        [3, 4, 2, 0, 1],\n",
    "#        [4, 3, 1, 2, 0]])\n",
    "\n",
    "# tutti 10 (no knn, gradient_boosting, quad_da, ada_boost, random_forest)\n",
    "# array([[4, 3, 2, 1, 0],\n",
    "#        [4, 3, 2, 1, 0],\n",
    "#        [4, 3, 2, 1, 0],\n",
    "#        [4, 3, 2, 1, 0],\n",
    "#        [4, 3, 2, 1, 0]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# soliti 4 classifiers\n",
    "# r i = 3.000\n",
    "# acc = 75.210\n",
    "\n",
    "# tutti 10\n",
    "# r i = 3.156\n",
    "# acc = 74.545\n",
    "\n",
    "# tutti 10 (no knn, gradient_boosting, quad_da, ada_boost, random_forest)\n",
    "# r i = 0.000\n",
    "# acc = 73.532"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(f\"\"\"\n",
    "# Accuracies:\n",
    "# Random Forest: {(accuracies_filtered_together_LOCO[-int(len(accuracies_filtered_together_LOCO) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][0]).round(2)}%\n",
    "# Logistic Regression: {(accuracies_filtered_together_LOCO[-int(len(accuracies_filtered_together_LOCO) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][1]).round(2)}%\n",
    "# Gradient Boosting: {(accuracies_filtered_together_LOCO[-int(len(accuracies_filtered_together_LOCO) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][2]).round(2)}%\n",
    "# SVM: {(accuracies_filtered_together_LOCO[-int(len(accuracies_filtered_together_LOCO) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][3]).round(2)}%\n",
    "# k Nearest Neighbors: {(accuracies_filtered_together_LOCO[-int(len(accuracies_filtered_together_LOCO) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][4]).round(2)}%\n",
    "# Nayve Bayes: {(accuracies_filtered_together_LOCO[-int(len(accuracies_filtered_together_LOCO) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][5]).round(2)}%\n",
    "# Linear Discriminant Analysis: {(accuracies_filtered_together_LOCO[-int(len(accuracies_filtered_together_LOCO) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][6]).round(2)}%\n",
    "# Quadratic Discriminnt Analysis: {(accuracies_filtered_together_LOCO[-int(len(accuracies_filtered_together_LOCO) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][7]).round(2)}%\n",
    "# AdaBoost: {(accuracies_filtered_together_LOCO[-int(len(accuracies_filtered_together_LOCO) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][8]).round(2)}%\n",
    "# histogram-based Gradient Boosting: {(accuracies_filtered_together_LOCO[-int(len(accuracies_filtered_together_LOCO) * (1-PERCENTAGE_FEATURE_TO_REMOVE))][9]).round(2)}%\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "LOCO_lead = np.array([\n",
    "\t# LOCO_random_forest_importances,\n",
    "\tLOCO_logistic_regression_importances,\n",
    "\t# LOCO_gradient_boosting_importances,\n",
    "\tLOCO_svm_importances,\n",
    "\t# LOCO_knn_importances,\n",
    "\tLOCO_cat_nb_importances,\n",
    "\tLOCO_line_da_importances,\n",
    "\t# LOCO_quad_da_importances,\n",
    "\t# LOCO_ada_boost_importances,\n",
    "\tLOCO_hist_gb_importances\n",
    "])\n",
    "LOCO_lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# robustness index\n",
    "LOCO_robustness_score = leaderboards_to_importance(LOCO_lead)\n",
    "LOCO_robustness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#  LOCO accuracy\n",
    "index = int(X.shape[-1] - np.round(original_dataset_size - original_dataset_size * PERCENTAGE_FEATURE_TO_REMOVE))\n",
    "LOCO_accuracy = np.array(accuracies_filtered_together_LOCO)[index].mean()\n",
    "LOCO_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Permutation Importance')\n",
    "print('r i : ', PI_robustness_score)\n",
    "print('acc : ', PI_accuracy)\n",
    "print('Leave one Covariate Out')\n",
    "print('r i : ', LOCO_robustness_score)\n",
    "print('acc : ', LOCO_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "hBR8ph1wk7Ua",
    "outputId": "abcb1cc8-be42-4178-bc84-6c9929db62f0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#features selected at each step\n",
    "X_filtered = np.copy(X)\n",
    "feature_importance_PCA = []\n",
    "features_selected_at_each_step_PCA = [np.arange(0, X.shape[-1]).tolist()]\n",
    "features_importance_at_each_step_PCA = []\n",
    "for i in trange(X.shape[-1], 0, -1):\n",
    "  if i > 1 :\n",
    "    pca = PCA()\n",
    "    pca.fit(X_filtered)\n",
    "    loadings_PCA = calc_loadings(X_filtered, pca.transform(X_filtered))\n",
    "    importances_PCA = to_distribution(loadings_to_importance(loadings_PCA))\n",
    "    feature_importance_PCA.append(importances_PCA)\n",
    "    selected_features_PCA = np.argsort(importances_PCA)[::-1][:-1]\n",
    "    features_selected_at_each_step_PCA.append(selected_features_PCA)\n",
    "    X_filtered = X_filtered[:, selected_features_PCA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "indexes = np.arange(0, X.shape[-1])\n",
    "for i in range(int(np.round(int(X.shape[-1] - np.round(original_dataset_size - original_dataset_size * PERCENTAGE_FEATURE_TO_REMOVE))) + 1)):\n",
    "\tindexes = indexes[features_selected_at_each_step_PCA[i]]\n",
    "new_X_PCA = X[:,indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yY6kg9QKnElJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(random_state=0).fit(new_X_PCA, y)\n",
    "logistic_regression = LogisticRegression(random_state=0).fit(new_X_PCA, y)\n",
    "gradient_boosting = GradientBoostingClassifier(random_state=0).fit(new_X_PCA, y)\n",
    "svm = SVC(kernel = \"linear\",random_state=0).fit(new_X_PCA, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3gL0dGmnJM6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##importances for each model for the best features\n",
    "PCA_random_forest_importances = np.argsort(permutation_importance(random_forest, new_X_PCA, y, n_repeats=10, random_state=0).importances_mean)\n",
    "PCA_logistic_regression_importances = np.argsort(permutation_importance(logistic_regression, new_X_PCA, y, n_repeats=10, random_state=0).importances_mean)\n",
    "PCA_gradient_boosting_importances = np.argsort(permutation_importance(gradient_boosting, new_X_PCA, y, n_repeats=10, random_state=0).importances_mean)\n",
    "PCA_svm_importancess = np.argsort(permutation_importance(svm, new_X_PCA, y, n_repeats=10, random_state=0).importances_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PCA_lead = np.array([\n",
    "\tPCA_random_forest_importances,\n",
    "\tPCA_logistic_regression_importances,\n",
    "\tPCA_gradient_boosting_importances,\n",
    "\tPCA_svm_importancess\n",
    "])\n",
    "PCA_lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PCA_robustness_score = leaderboards_to_importance(PCA_lead)\n",
    "PCA_robustness_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYtpuvEZnc4c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### KernelPCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "pdpt4AC7n-SE",
    "outputId": "6aa1485d-24d2-4439-e83f-6a1907c3fc64",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#features selected at each step\n",
    "X_filtered = np.copy(X)\n",
    "feature_importance_KPCA = []\n",
    "features_selected_at_each_step_KPCA = [np.arange(0, X.shape[-1]).tolist()]\n",
    "features_importance_at_each_step_KPCA = []\n",
    "for i in trange(X.shape[-1], 0, -1):\n",
    "  if i > 1 :\n",
    "    kpca = KernelPCA(kernel=\"cosine\")\n",
    "    kpca.fit(X_filtered)\n",
    "    loadings_KPCA = calc_loadings(X_filtered, kpca.transform(X_filtered))\n",
    "    importances_KPCA = to_distribution(loadings_to_importance(loadings_KPCA))\n",
    "    feature_importance_KPCA.append(importances_KPCA)\n",
    "    selected_features_KPCA = np.argsort(importances_KPCA)[::-1][:-1]\n",
    "    features_selected_at_each_step_KPCA.append(selected_features_KPCA)\n",
    "    X_filtered = X_filtered[:, selected_features_KPCA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "wUnaVFVin7nY",
    "outputId": "05f7fd93-4859-46db-99a7-5dc50c5de644",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "indexes = np.arange(0, X.shape[-1])\n",
    "for i in range(int(np.round(int(X.shape[-1] - np.round(original_dataset_size - original_dataset_size * PERCENTAGE_FEATURE_TO_REMOVE))) + 1)):\n",
    "\tindexes = indexes[features_selected_at_each_step_KPCA[i]]\n",
    "new_X_KPCA = X[:,indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Id39B2AjsLqd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(random_state=0).fit(new_X_KPCA, y)\n",
    "logistic_regression = LogisticRegression(random_state=0).fit(new_X_KPCA, y)\n",
    "gradient_boosting = GradientBoostingClassifier(random_state=0).fit(new_X_KPCA, y)\n",
    "svm = SVC(kernel = \"linear\",random_state=0).fit(new_X_KPCA, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XyV1b5oEsfQ2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##importances for each model for the best features\n",
    "KPCA_random_forest_importances = np.argsort(permutation_importance(random_forest, new_X_KPCA, y, n_repeats=10, random_state=0).importances_mean)\n",
    "KPCA_logistic_regression_importances = np.argsort(permutation_importance(logistic_regression, new_X_KPCA, y, n_repeats=10, random_state=0).importances_mean)\n",
    "KPCA_gradient_boosting_importances = np.argsort(permutation_importance(gradient_boosting, new_X_KPCA, y, n_repeats=10, random_state=0).importances_mean)\n",
    "KPCA_svm_importances = np.argsort(permutation_importance(svm, new_X_KPCA, y, n_repeats=10, random_state=0).importances_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5kqxQgJsxK7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "KPCA_lead = np.array([\n",
    "\tKPCA_random_forest_importances,\n",
    "\tKPCA_logistic_regression_importances,\n",
    "\tKPCA_gradient_boosting_importances,\n",
    "\tKPCA_svm_importances\n",
    "])\n",
    "KPCA_lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "KPCA_robustness_score = leaderboards_to_importance(KPCA_lead)\n",
    "KPCA_robustness_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrSqw-_JsyF4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sparse PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zWpa291as-uI",
    "outputId": "bc447392-2efe-4334-b553-61f9cd5a29e4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#features selected at each step\n",
    "X_filtered = np.copy(X)\n",
    "feature_importance_SPCA = []\n",
    "features_selected_at_each_step_SPCA = [np.arange(0, X.shape[-1]).tolist()]\n",
    "features_importance_at_each_step_SPCA = []\n",
    "for i in trange(X.shape[-1], 0, -1):\n",
    "  if i > 1 :\n",
    "    sparse_pca = SparsePCA()\n",
    "    sparse_pca.fit(X_filtered)\n",
    "    loadings_SPCA = calc_loadings(X_filtered, sparse_pca.transform(X_filtered))\n",
    "    importances_SPCA = to_distribution(loadings_to_importance(loadings_SPCA))\n",
    "    feature_importance_SPCA.append(importances_SPCA)\n",
    "    selected_features_SPCA = np.argsort(importances_SPCA)[::-1][:-1]\n",
    "    features_selected_at_each_step_SPCA.append(selected_features_SPCA)\n",
    "    X_filtered = X_filtered[:, selected_features_SPCA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xv8MjgYAs4TG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "indexes = np.arange(0, X.shape[-1])\n",
    "for i in range(int(np.round(int(X.shape[-1] - np.round(original_dataset_size - original_dataset_size * PERCENTAGE_FEATURE_TO_REMOVE))) + 1)):\n",
    "\tindexes = indexes[features_selected_at_each_step_SPCA[i]]\n",
    "new_X_SPCA = X[:,indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PdtRSg3ftznh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(random_state=0).fit(new_X_SPCA, y)\n",
    "logistic_regression = LogisticRegression(random_state=0).fit(new_X_SPCA, y)\n",
    "gradient_boosting = GradientBoostingClassifier(random_state=0).fit(new_X_SPCA, y)\n",
    "svm = SVC(kernel = \"linear\",random_state=0).fit(new_X_SPCA, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_P5Kvk-t11i",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#features importances\n",
    "SPCA_random_forest_importances = np.argsort(permutation_importance(random_forest, new_X_SPCA, y, n_repeats=10, random_state=0).importances_mean)\n",
    "SPCA_logistic_regression_importances = np.argsort(permutation_importance(logistic_regression, new_X_SPCA, y, n_repeats=10, random_state=0).importances_mean)\n",
    "SPCA_gradient_boosting_importances = np.argsort(permutation_importance(gradient_boosting, new_X_SPCA, y, n_repeats=10, random_state=0).importances_mean)\n",
    "SPCA_svm_importances = np.argsort(permutation_importance(svm, new_X_SPCA, y, n_repeats=10, random_state=0).importances_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SPCA_lead = np.array([\n",
    "\tSPCA_random_forest_importances,\n",
    "\tSPCA_logistic_regression_importances,\n",
    "\tSPCA_gradient_boosting_importances,\n",
    "\tSPCA_svm_importances\n",
    "])\n",
    "SPCA_lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SPCA_robustness_score = leaderboards_to_importance(SPCA_lead)\n",
    "SPCA_robustness_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e87rxIcAuNtS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXMGHy51uVSk",
    "outputId": "c00722d5-92a8-4575-e2be-350b6ae73d2c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#features selected at each step\n",
    "X_filtered = np.copy(X)\n",
    "feature_importance_TSVD = []\n",
    "features_selected_at_each_step_TSVD = [np.arange(0, X.shape[-1]).tolist()]\n",
    "features_importance_at_each_step_TSVD = []\n",
    "for i in trange(X.shape[-1]-1, 0, -1):\n",
    "  if i > 1 :\n",
    "    truncated_svd = TruncatedSVD()\n",
    "    truncated_svd.fit(X_filtered)\n",
    "    loadings_TSVD = calc_loadings(X_filtered, truncated_svd.transform(X_filtered))\n",
    "    importances_TSVD = to_distribution(loadings_to_importance(loadings_TSVD))\n",
    "    feature_importance_TSVD.append(importances_TSVD)\n",
    "    selected_features_TSVD = np.argsort(importances_TSVD)[::-1][:-1]\n",
    "    features_selected_at_each_step_TSVD.append(selected_features_TSVD)\n",
    "    X_filtered = X_filtered[:, selected_features_TSVD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MaF5PORKvb8E",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "indexes = np.arange(0, X.shape[-1])\n",
    "for i in range(int(np.round(int(X.shape[-1] - np.round(original_dataset_size - original_dataset_size * PERCENTAGE_FEATURE_TO_REMOVE))) + 1)):\n",
    "\tindexes = indexes[features_selected_at_each_step_TSVD[i]]\n",
    "new_X_TSVD = X[:,indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhmaFsapwFl-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(random_state=0).fit(new_X_TSVD, y)\n",
    "logistic_regression = LogisticRegression(random_state=0).fit(new_X_TSVD, y)\n",
    "gradient_boosting = GradientBoostingClassifier(random_state=0).fit(new_X_TSVD, y)\n",
    "svm = SVC(kernel = \"linear\",random_state=0).fit(new_X_TSVD, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NaPACM8av3IO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#features importances\n",
    "TSVD_random_forest_importances = np.argsort(permutation_importance(random_forest, new_X_TSVD, y, n_repeats=10, random_state=0).importances_mean)\n",
    "TSVD_logistic_regression_importances = np.argsort(permutation_importance(logistic_regression, new_X_TSVD, y, n_repeats=10, random_state=0).importances_mean)\n",
    "TSVD_gradient_boosting_importances = np.argsort(permutation_importance(gradient_boosting, new_X_TSVD, y, n_repeats=10, random_state=0).importances_mean)\n",
    "TSVD_svm_importances = np.argsort(permutation_importance(svm, new_X_TSVD, y, n_repeats=10, random_state=0).importances_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TSVD_lead = np.array([\n",
    "\tTSVD_random_forest_importances,\n",
    "\tTSVD_logistic_regression_importances,\n",
    "\tTSVD_gradient_boosting_importances,\n",
    "\tTSVD_svm_importances\n",
    "])\n",
    "TSVD_lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TSVD_robustness_score = leaderboards_to_importance(TSVD_lead)\n",
    "TSVD_robustness_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ok8e-5TwgGt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Factor analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fzjnxJq4v3Ps",
    "outputId": "760b524b-c590-4fd0-dd90-7b9029a33a91",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#features selected at each step\n",
    "X_filtered = np.copy(X)\n",
    "feature_importance_FA = []\n",
    "features_selected_at_each_step_FA = [np.arange(0, X.shape[-1]).tolist()]\n",
    "features_importance_at_each_step_FA = []\n",
    "for i in trange(X.shape[-1]-1, 0, -1):\n",
    "  if i > 1 :\n",
    "    factor_analysis = FactorAnalysis()\n",
    "    factor_analysis.fit(X_filtered)\n",
    "    loadings_FA = calc_loadings(X_filtered, factor_analysis.transform(X_filtered))\n",
    "    importances_FA = to_distribution(loadings_to_importance(loadings_FA))\n",
    "    feature_importance_FA.append(importances_FA)\n",
    "    selected_features_FA = np.argsort(importances_FA)[::-1][:-1]\n",
    "    features_selected_at_each_step_FA.append(selected_features_FA)\n",
    "    X_filtered = X_filtered[:, selected_features_FA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qq6Xost5v3WI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "indexes = np.arange(0, X.shape[-1])\n",
    "for i in range(int(np.round(int(X.shape[-1] - np.round(original_dataset_size - original_dataset_size * PERCENTAGE_FEATURE_TO_REMOVE))) + 1)):\n",
    "\tindexes = indexes[features_selected_at_each_step_FA[i]]\n",
    "new_X_FA = X[:,indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6KaYh9XxcSL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(random_state=0).fit(new_X_FA, y)\n",
    "logistic_regression = LogisticRegression(random_state=0).fit(new_X_FA, y)\n",
    "gradient_boosting = GradientBoostingClassifier(random_state=0).fit(new_X_FA, y)\n",
    "svm = SVC(kernel = \"linear\",random_state=0).fit(new_X_FA, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gl7mHjnfxcV4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#features importances\n",
    "FA_random_forest_importances = np.argsort(permutation_importance(random_forest, new_X_FA, y, n_repeats=10, random_state=0).importances_mean)\n",
    "FA_logistic_regression_importances = np.argsort(permutation_importance(logistic_regression, new_X_FA, y, n_repeats=10, random_state=0).importances_mean)\n",
    "FA_gradient_boosting_importances = np.argsort(permutation_importance(gradient_boosting, new_X_FA, y, n_repeats=10, random_state=0).importances_mean)\n",
    "FA_svm_importances = np.argsort(permutation_importance(svm, new_X_FA, y, n_repeats=10, random_state=0).importances_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "FA_lead = np.array([\n",
    "\tFA_random_forest_importances,\n",
    "\tFA_logistic_regression_importances,\n",
    "\tFA_gradient_boosting_importances,\n",
    "\tFA_svm_importances\n",
    "])\n",
    "FA_lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "FA_robustness_score = leaderboards_to_importance(FA_lead)\n",
    "FA_robustness_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9P3dnWZNxxcy",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Relief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F-EK43kMxcZk",
    "outputId": "872db756-e682-462b-eb4b-298b511431ae",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#features selected at each step\n",
    "X_filtered = np.copy(X)\n",
    "feature_importance_R = []\n",
    "features_selected_at_each_step_R = [np.arange(0, X.shape[-1]).tolist()]\n",
    "features_importance_at_each_step_R = []\n",
    "for i in trange(X.shape[-1], 0, -1):\n",
    "  if i > 1 :\n",
    "    relief = ReliefF(n_neighbors=5).fit(X_filtered,y)\n",
    "    importances_R=to_distribution(relief.feature_importances_)\n",
    "    feature_importance_R.append(importances_R)\n",
    "    selected_features_R = np.argsort(importances_R)[::-1][:-1]\n",
    "    features_selected_at_each_step_R.append(selected_features_R)\n",
    "    X_filtered = X_filtered[:, selected_features_R]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xexXYM13xcc9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "indexes = np.arange(0, X.shape[-1])\n",
    "for i in range(int(np.round(int(X.shape[-1] - np.round(original_dataset_size - original_dataset_size * PERCENTAGE_FEATURE_TO_REMOVE))) + 1)):\n",
    "\tindexes = indexes[features_selected_at_each_step_R[i]]\n",
    "new_X_R = X[:,indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDaiBON-zlzo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(random_state=0).fit(new_X_R, y)\n",
    "logistic_regression = LogisticRegression(random_state=0).fit(new_X_R, y)\n",
    "gradient_boosting = GradientBoostingClassifier(random_state=0).fit(new_X_R, y)\n",
    "svm = SVC(kernel = \"linear\",random_state=0).fit(new_X_R, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7L-P2u7zl5z",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#features importances\n",
    "R_random_forest_importances = np.argsort(permutation_importance(random_forest, new_X_R, y, n_repeats=10, random_state=0).importances_mean)\n",
    "R_logistic_regression_importances = np.argsort(permutation_importance(logistic_regression, new_X_R, y, n_repeats=10, random_state=0).importances_mean)\n",
    "R_gradient_boosting_importances = np.argsort(permutation_importance(gradient_boosting, new_X_R, y, n_repeats=10, random_state=0).importances_mean)\n",
    "R_svm_importances = np.argsort(permutation_importance(svm, new_X_R, y, n_repeats=10, random_state=0).importances_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "R_lead = np.array([\n",
    "\tR_random_forest_importances,\n",
    "\tR_logistic_regression_importances,\n",
    "\tR_gradient_boosting_importances,\n",
    "\tR_svm_importances\n",
    "])\n",
    "R_lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "R_robustness_score = leaderboards_to_importance(R_lead)\n",
    "R_robustness_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "index = int(X.shape[-1] - np.round(original_dataset_size - original_dataset_size * PERCENTAGE_FEATURE_TO_REMOVE))\n",
    "accuracies_filtered_together_PI = np.array(accuracies_filtered_together_PI)\n",
    "accuracies_filtered_together_LOCO = np.array(accuracies_filtered_together_LOCO)\n",
    "\n",
    "\n",
    "result = np.array([\n",
    "\t[\n",
    "\t\tPI_robustness_score,\n",
    "\t\tLOCO_robustness_score,\n",
    "\t\tPCA_robustness_score,\n",
    "\t\tKPCA_robustness_score,\n",
    "\t\tSPCA_robustness_score,\n",
    "\t\tTSVD_robustness_score,\n",
    "\t\tFA_robustness_score,\n",
    "\t\tR_robustness_score,\n",
    "\t],[\n",
    "\t\taccuracies_filtered_together_PI[index].mean(),\n",
    "\t\taccuracies_filtered_together_LOCO[index].mean(),\n",
    "\t\taccuracies_filtered_PCA_BS[index].mean(),\n",
    "\t\taccuracies_filtered_KPCA[index].mean(),\n",
    "\t\taccuracies_filtered_SparsePCA[index].mean(),\n",
    "\t\taccuracies_filtered_trunc_SVD[index].mean(),\n",
    "\t\taccuracies_filtered_factor_analysis[index].mean(),\n",
    "\t\taccuracies_with_relief_selection[index].mean(),\n",
    "\t]\n",
    "])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "At the first preliminary meeting the following have been concluded:\n",
    "- makes no sense to test this method on \"too simple\" datasets, aka those with intrinsic high correlation inside them\n",
    "- the following are the best techniques:\n",
    "\t- BS-LOCO\n",
    "\t- BS-FA / BS-Relief\n",
    "- apply those best techniques to all the models available\n",
    "- check if the \"reason of lying\" affects \"how do you lie\" (the )\n",
    "- using an autoencoder with permutation importance as \"FS with model independent without classifiers\", and testing on non-linear classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.api._v2.keras as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_one_hot = K.utils.to_categorical(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "K.models.Sequential([\n",
    "\tK.layers.Flatten(input_shape=X_one_hot.shape[1:]),\n",
    "\tK.layers.Dense(64, activation=tf.nn.leaky_relu)\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
